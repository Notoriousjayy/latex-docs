\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{amsmath,amssymb}
\usepackage{geometry}
\usepackage{microtype}
\usepackage{hyperref}

\geometry{margin=1in}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  citecolor=blue
}

\title{How Matrices and Polynomials Relate to Algebraic Operations}
\author{}
\date{}

\begin{document}

\maketitle

\section{Introduction}

Matrices and polynomials are deeply connected through the idea that a matrix can play the role of the ``variable'' in a polynomial. Many standard constructions in linear algebra, such as characteristic polynomials and eigenvalues, are built directly from polynomial algebra.

This document summarizes the main ways matrices and polynomials interact and how algebraic operations on one side translate to the other.

\section{Matrix Polynomials}

Let
\[
p(x) = a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n
\]
be a polynomial with scalar coefficients, and let \(A\) be a square matrix. We can define the \emph{matrix polynomial}
\[
p(A) = a_0 I + a_1 A + a_2 A^2 + \dots + a_n A^n,
\]
where \(I\) is the identity matrix of the same size as \(A\).

Key points:
\begin{itemize}
  \item The usual operations on polynomials (addition, scalar multiplication, multiplication) extend naturally to matrix polynomials.
  \item If \(p(A) = 0\) (the zero matrix), then we say that \(A\) is a \emph{root} of the polynomial \(p\) in the matrix sense.
  \item This viewpoint allows us to use polynomial algebra to reason about linear transformations, since matrices represent linear maps.
\end{itemize}

\section{Characteristic Polynomials and Eigenvalues}

For a square matrix \(A \in \mathbb{R}^{n \times n}\) (or \(\mathbb{C}^{n \times n}\)), the \emph{characteristic polynomial} is defined by
\[
\chi_A(\lambda) = \det(\lambda I - A).
\]

This construction links matrix theory to polynomial theory in several important ways:
\begin{itemize}
  \item The \emph{eigenvalues} of \(A\) are exactly the \emph{roots} of \(\chi_A(\lambda)\).
  \item Up to sign, the constant term of \(\chi_A(\lambda)\) is the determinant of \(A\), and the coefficient of \(\lambda^{n-1}\) is (minus) the trace of \(A\).
  \item The \emph{Cayley--Hamilton theorem} states that
  \[
    \chi_A(A) = 0,
  \]
  meaning the matrix \(A\) satisfies its own characteristic polynomial. This is a concrete example of a polynomial identity involving a matrix as the ``variable.''
\end{itemize}

Thus, characteristic polynomials provide an algebraic bridge from matrix invariants (like eigenvalues, determinant, trace) to polynomial roots and coefficients.

\section{Companion Matrices}

The connection also works in the opposite direction: starting from a polynomial, we can build a matrix.

Consider a monic polynomial of degree \(n\),
\[
p(x) = x^n + c_{n-1} x^{n-1} + \dots + c_1 x + c_0.
\]
We can construct its \emph{companion matrix} \(C \in \mathbb{R}^{n \times n}\) (or \(\mathbb{C}^{n \times n}\)) of the form
\[
C =
\begin{pmatrix}
0      & 0      & \dots  & 0      & -c_0 \\
1      & 0      & \dots  & 0      & -c_1 \\
0      & 1      & \dots  & 0      & -c_2 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
0      & 0      & \dots  & 1      & -c_{n-1}
\end{pmatrix}.
\]

This matrix has the property that its characteristic polynomial is exactly \(p(x)\). Consequently:
\begin{itemize}
  \item The eigenvalues of \(C\) are precisely the roots of \(p(x)\).
  \item The problem of finding polynomial roots can be reformulated as an eigenvalue problem for a matrix.
\end{itemize}

In this sense, polynomials and matrices encode the same spectral information in different formats.

\section{Polynomials as Vectors and Linear Algebra on Polynomial Families}

Another perspective is to treat polynomials themselves as elements of a vector space. A polynomial
\[
p(x) = a_0 + a_1 x + \dots + a_n x^n
\]
can be represented by its coefficient vector
\[
(a_0, a_1, \dots, a_n) \in \mathbb{R}^{n+1} \quad \text{(or } \mathbb{C}^{n+1}\text{)}.
\]

This leads to the following observations:
\begin{itemize}
  \item The set of all polynomials of degree at most \(n\) forms a vector space of dimension \(n+1\).
  \item A collection of polynomials can be organized into a matrix whose columns (or rows) are coefficient vectors.
  \item Questions like \emph{linear independence} of polynomials or \emph{spanning sets} can then be studied using standard linear algebra tools: rank, row-reduction, null spaces, and so on.
\end{itemize}

Thus, linear algebra applies directly to families of polynomials via their coefficient representations.

\section{Big-Picture Summary}

The relationship between matrices and polynomials can be summarized as follows:
\begin{itemize}
  \item \textbf{Polynomials as functions on matrices:} Given a polynomial \(p(x)\), we can evaluate it at a matrix \(A\) to obtain \(p(A)\). This defines matrix polynomials and leads to identities like the Cayley--Hamilton theorem.
  \item \textbf{Matrices as sources of polynomials:} From a matrix \(A\), we derive the characteristic polynomial \(\chi_A(\lambda)\), whose roots are the eigenvalues of \(A\). The minimal polynomial also captures essential information about the structure of \(A\).
  \item \textbf{Encoding polynomials as matrices:} Using companion matrices, a single polynomial can be encoded in a matrix whose eigenvalues are the polynomial's roots.
  \item \textbf{Polynomials as vectors:} By treating polynomials as coefficient vectors, we can apply all of linear algebra's machinery (rank, bases, linear transformations) directly to sets of polynomials.
\end{itemize}

In short, matrices and polynomials are two tightly linked algebraic objects: polynomials act on matrices, matrices generate polynomials, and both live in vector spaces where linear algebra provides a common language for analysis.

\end{document}

