\documentclass[11pt]{article}

% ---- Packages ----
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{array}
\usepackage{ragged2e}
\usepackage{enumitem}
\usepackage{hyperref}
\hypersetup{
  colorlinks=true,
  linkcolor=blue!60!black,
  urlcolor=blue!60!black,
  citecolor=blue!60!black,
  pdftitle={LLM Design Patterns — User Story Template and Guide},
  pdfauthor={},
  pdfsubject={User Story Template},
  pdfcreator={LaTeX}
}

% tcolorbox with "most" library to ensure stable internals
\usepackage[most]{tcolorbox}

% ---- Colors ----
\definecolor{cardframe}{RGB}{33, 37, 41} % dark gray
\definecolor{cardback}{RGB}{248, 249, 250} % near white
\definecolor{accent}{RGB}{52, 120, 199}    % blue
\definecolor{accentbg}{RGB}{235, 244, 255} % light blue
\definecolor{warnbg}{RGB}{255, 245, 230}   % light orange
\definecolor{warndef}{RGB}{197, 98, 0}     % orange

% ---- Small helpers (named uniquely to avoid clashes) ----
\newtcbox{\uspill}[1][]{nobeforeafter, tcbox raise base,
  enhanced, boxsep=1pt, left=2pt, right=2pt, top=1pt, bottom=1pt,
  colframe=accent, colback=accentbg, boxrule=0.5pt, arc=2pt, #1}

\newcommand{\usbadge}[1]{\uspill{\footnotesize #1}}

% Definitions of DoR/DoD as text blocks
\newcommand{\USDoR}{\textbf{Definition of Ready:} Persona clear; Acceptance template drafted; Dependencies known; Estimate set.}
\newcommand{\USDoD}{\textbf{Definition of Done:} Tests pass; Acceptance criteria met; Quality checks; Docs updated; Deployed or feature flagged.}

% ---- Boxed environments ----
% A story card that shows key metadata and provides slots for lists.
\newenvironment{StoryCard}[2][]{%
  \begin{tcolorbox}[enhanced, breakable, sharp corners=all, colback=cardback, colframe=cardframe,
    title={#2}, fonttitle=\bfseries\large, boxrule=0.6pt, borderline west={2pt}{0pt}{accent}, #1]
}{%
  \end{tcolorbox}
}

% A tasks box that ALWAYS wraps an itemize list so users can simply \item ...

% ---- BEGIN: Exact TasksBox definition (inserted) ----
\makeatletter
% -------------------- Story Card (exact screenshot style) --------------------
% Dependencies: tabularx, array, ragged2e, tcolorbox[most], xcolor, amsmath/amssymb
% Uses colors and \tcbset you already have.

% Safe helpers (only define if missing)
\providecommand{\cb}{\(\square\)}
\providecommand{\DoR}{\textbf{Definition of Ready:} Persona clear; AC drafted; Dependencies known; Estimate set.}
\providecommand{\DoD}{\textbf{Definition of Done:} All ACs pass; Tests green; Security/a11y checks; Docs updated; Deployed/flagged.}
\providecommand{\badge}[1]{\fbox{\footnotesize #1}} % will be overridden by pill if present
\@ifundefined{pill}{%
  \newtcbox{\pill}{on line, arc=3pt, boxsep=0.8pt, left=4pt,right=4pt,top=1pt,bottom=1pt,
    colframe=gray!50, colback=gray!15, boxrule=0.3pt}%
  \renewcommand{\badge}[1]{\pill{\footnotesize #1}}%
}{}

% A label column type that matches the screenshot (bold, right-aligned)
\newcolumntype{Lbl}[1]{>{\raggedleft\arraybackslash\bfseries}p{#1}}
\newcolumntype{Y}{>{\RaggedRight\arraybackslash}X} % ragged-right content

% StoryCard macro (9 args)
% 1: ID   2: Title   3: Epic/Feature   4: Business Value
% 5: Priority   6: Estimate(SP)   7: Persona   8: Dependencies   9: Assumptions/Risks


% -------------------- Tasks box (matches screenshot) --------------------

% ---- BEGIN: Exact TasksBox definition (inserted) ----
\makeatletter
% -------------------- Story Card (exact screenshot style) --------------------
% Dependencies: tabularx, array, ragged2e, tcolorbox[most], xcolor, amsmath/amssymb
% Uses colors and \tcbset you already have.

% Safe helpers (only define if missing)
\providecommand{\cb}{\(\square\)}
\providecommand{\DoR}{\textbf{Definition of Ready:} Persona clear; AC drafted; Dependencies known; Estimate set.}
\providecommand{\DoD}{\textbf{Definition of Done:} All ACs pass; Tests green; Security/a11y checks; Docs updated; Deployed/flagged.}
\providecommand{\badge}[1]{\fbox{\footnotesize #1}} % will be overridden by pill if present
\@ifundefined{pill}{%
  \newtcbox{\pill}{on line, arc=3pt, boxsep=0.8pt, left=4pt,right=4pt,top=1pt,bottom=1pt,
    colframe=gray!50, colback=gray!15, boxrule=0.3pt}%
  \renewcommand{\badge}[1]{\pill{\footnotesize #1}}%
}{}

% A label column type that matches the screenshot (bold, right-aligned)

% StoryCard macro (9 args)
% 1: ID   2: Title   3: Epic/Feature   4: Business Value
% 5: Priority   6: Estimate(SP)   7: Persona   8: Dependencies   9: Assumptions/Risks


% -------------------- Tasks box (matches screenshot) --------------------
\newenvironment{TasksBox}[1][Tasks]{%
  \begin{tcolorbox}[
    enhanced,breakable,
    colback=gray!1, colframe=gray!35,
    colbacktitle=gray!6, coltitle=black,
    title={#1}, fonttitle=\bfseries,
    borderline west={1.8pt}{0pt}{MidnightBlue},
    arc=2pt, boxrule=0.4pt,
    left=10pt,right=10pt,top=6pt,bottom=6pt,
    before skip=6pt, after skip=10pt
  ]
  \small
  \begin{itemize}[
    label=\cb, leftmargin=*, labelsep=0.6em,
    itemsep=4pt, topsep=2pt, parsep=0pt
  ]
}{%
  \end{itemize}
  \end{tcolorbox}
}
\makeatother
% ---- END: Exact TasksBox definition ----

\makeatother
% ---- END: Exact TasksBox definition ----

% Convenience label macro to print bold left labels
\newcommand{\Field}[1]{\noindent\textbf{#1}\ }

% ---- Document ----
\begin{document}

\begin{center}
  {\LARGE \textbf{LLM Design Patterns — Study Plan \& User Story Template}}\\[4pt]
  \usbadge{Template} \quad \usbadge{User Stories} \quad \usbadge{LLM Projects}
\end{center}

\vspace{6pt}
\tableofcontents
\vspace{10pt}

\section*{How to Use This Template}
\addcontentsline{toc}{section}{How to Use This Template}
\begin{itemize}[itemsep=2pt, topsep=3pt]
  \item \textbf{ID:} Stable, unique identifier (e.g., LLMDP-CH26-01).
  \item \textbf{Title:} Action-oriented, outcome-focused (e.g., ``Build a minimal RAG pipeline'').
  \item \textbf{Epic / Feature:} Chapter-level capability (e.g., ``Ch26: RAG Patterns'' $\rightarrow$ ``RAG Quickstart'').
  \item \textbf{Business Value:} Why this matters (e.g., reduces hallucinations on internal docs).
  \item \textbf{Priority / Estimate:} MoSCoW or P1--P4, and rough SP.
  \item \textbf{Persona:} Learner / Engineer / Reviewer / Sponsor.
  \item \textbf{Dependencies:} Pre-req chapters, tooling, data.
  \item \textbf{Assumptions / Risks:} Known constraints, open questions.
  \item \textbf{Acceptance Criteria:} Given/When/Then, objective and testable.
  \item \textbf{Evidence:} Links to artifacts (code, reports, dashboards).
\end{itemize}

\section*{User Story Template}
\addcontentsline{toc}{section}{User Story Template}
\begin{StoryCard}{Story Card Definition}
  \Field{Definition of Ready.} \USDoR\par\smallskip
  \Field{Definition of Done.} \USDoD\par\medskip

  \Field{Guidelines.}
  \begin{itemize}[itemsep=2pt, topsep=2pt]
    \item \textbf{INVEST:} Independent, Negotiable, Valuable, Estimable, Small, Testable.
    \item \textbf{Format:} \emph{As a [persona], I want [capability], so that [outcome].}
    \item \textbf{Acceptance (BDD):} Use 3--6 clear Given/When/Then criteria.
    \item \textbf{Evidence:} Prefer measurable artifacts (metrics, dashboards, traces).
  \end{itemize}
\end{StoryCard}

\begin{StoryCard}{User Story Template}
  \Field{ID:} \texttt{LLMDP-XXX}\par
  \Field{Title:} \textit{Short, imperative headline}\par
  \Field{Epic / Feature:} \textit{Chapter / Capability}\par
  \Field{Business Value:} \textit{Why this matters}\par
  \Field{Priority / Estimate:} \textit{P1--P4}, \textit{N story points}\par
  \Field{Persona:} \textit{Learner / Engineer / Reviewer / Sponsor}\par
  \Field{Dependencies:} \textit{Prereqs and tooling}\par
  \Field{Assumptions / Risks:} \textit{Constraints, open questions}\par
  \medskip

  \begin{TasksBox}[User Story]
    \item As a [\textbf{persona}], I want [\textbf{capability}] so that [\textbf{outcome}].
  \end{TasksBox}

  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} [precondition], \textbf{when} [action], \textbf{then} [result].
    \item \textbf{Given} [another precondition], \textbf{when} [action], \textbf{then} [result].
  \end{TasksBox}

  \begin{TasksBox}[Evidence to Attach]
    \item [Code/notebook link]; [metrics.json]; [dashboard/screenshot].
    \item [Risks, notes, and follow-ups].
  \end{TasksBox}
\end{StoryCard}

\section*{Example Cards}
\addcontentsline{toc}{section}{Example Cards}

% --- Example 1 ---
\begin{StoryCard}{CH26 Example — Minimal RAG Quickstart}
  \Field{ID:} LLMDP-CH26-01\par
  \Field{Title:} Build a minimal RAG pipeline\par
  \Field{Epic / Feature:} Chapter 26 — RAG Patterns / Quickstart\par
  \Field{Business Value:} Enable retrieval-grounded answers to reduce hallucination rate on internal docs.\par
  \Field{Priority / Estimate:} P1, 5 SP\par
  \Field{Persona:} Learner\par
  \Field{Dependencies:} Document corpus; embedding model; vector store.\par
  \Field{Assumptions / Risks:} Cost of embeddings; evaluation data availability.\par
  \medskip

  \begin{TasksBox}[User Story]
    \item As a learner, I want to build a minimal RAG pipeline (ingest, index, retrieve, answer) so that I can baseline grounded answering on a small internal corpus.
  \end{TasksBox}

  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} a seed corpus and 10 evaluation questions, \textbf{when} I run the baseline pipeline, \textbf{then} I produce answers and store traces.
    \item \textbf{Given} the same questions, \textbf{when} I compare RAG vs. no-RAG on exact-match, \textbf{then} RAG improves accuracy by $\geq$ 20\%.
    \item \textbf{Given} the index is rebuilt, \textbf{when} I rerun evaluation, \textbf{then} results are reproducible within 2\% variance.
  \end{TasksBox}

  \begin{TasksBox}[Evidence to Attach]
    \item Notebook and CLI for ingest/index/query; config in repo.
    \item Evaluation report (\texttt{metrics.json}) with factuality, precision/recall, latency, cost.
    \item README shows setup, commands, and limitations.
  \end{TasksBox}
\end{StoryCard}

% --- Example 2 ---
\begin{StoryCard}{CH17 Example — Chain-of-Thought and Self-Consistency}
  \Field{ID:} LLMDP-CH17-02\par
  \Field{Title:} Evaluate CoT + self-consistency on a math/logic set\par
  \Field{Epic / Feature:} Chapter 17 — Prompting Patterns / Reasoning\par
  \Field{Business Value:} Quantify accuracy/cost trade-offs for CoT and sampling strategies.\par
  \Field{Priority / Estimate:} P2, 8 SP\par
  \Field{Persona:} QA reviewer\par
  \Field{Dependencies:} Prompting framework; evaluation set; sampling controls.\par
  \Field{Assumptions / Risks:} Longer latency and sampling cost; leakage risk if traces are exposed.\par
  \medskip

  \begin{TasksBox}[User Story]
    \item As a QA reviewer, I want CoT and self-consistency variants in evaluation so that we can decide whether to enable them by default.
  \end{TasksBox}

  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} a 20-task set, \textbf{when} I enable CoT with $k=1$, \textbf{then} accuracy improves by $\geq$ 8\% vs. baseline at $\leq$ 2$\times$ latency.
    \item \textbf{Given} the same set, \textbf{when} I run single-shot vs. self-consistency with $k=5$, \textbf{then} I report accuracy and cost-per-correct with plots.
    \item \textbf{Given} sensitive tasks, \textbf{when} I log traces, \textbf{then} PII is redacted and storage complies with policy.
  \end{TasksBox}

  \begin{TasksBox}[Evidence to Attach]
    \item Experiment configs checked in; results tracked in \texttt{metrics.json}.
    \item Plots in \texttt{reports/} and a short \texttt{cot\_findings.md}.
  \end{TasksBox}
\end{StoryCard}

% =========================================================
%                Chapter User Stories (1--30)
% =========================================================
\section*{Chapter User Stories}
\addcontentsline{toc}{section}{Chapter User Stories}

% ---------- Ch01 ----------
\begin{StoryCard}{CH01 — Pattern Selection Rubric \& Success Criteria}
  \Field{ID:} LLMDP-CH01-01\par
  \Field{Title:} Create an LLM pattern selection rubric with KPIs\par
  \Field{Epic / Feature:} Chapter 1 — Introduction / Pattern Mindset\par
  \Field{Business Value:} Aligns solutions to constraints (cost, latency, safety) and reduces rework.\par
  \Field{Priority / Estimate:} P1, 3 SP\par
  \Field{Persona:} Architect / Tech Lead\par
  \Field{Dependencies:} None; access to product goals and constraints.\par
  \Field{Assumptions / Risks:} Stakeholder alignment required; scope creep risk.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an architect, I want a rubric mapping use cases to LLM patterns so that the team can consistently choose low-risk, high-value approaches.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} top 5 use cases, \textbf{when} I score each against constraints (cost, latency, safety, accuracy), \textbf{then} the rubric recommends 1--2 patterns with rationale.
    \item \textbf{Given} a new use case, \textbf{when} I apply the rubric, \textbf{then} I can produce KPIs (quality, cost/request, p95 latency) and a validation plan.
    \item \textbf{Given} conflicting constraints, \textbf{when} I document trade-offs, \textbf{then} stakeholders sign off on chosen pattern.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item \texttt{patterns\_rubric.md}; \texttt{kpi\_matrix.csv}; architecture decision record (ADR).
    \item Slide summary with decision tree.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch02 ----------
\begin{StoryCard}{CH02 — Data Cleaning Pipeline}
  \Field{ID:} LLMDP-CH02-01\par
  \Field{Title:} Implement reproducible data cleaning (dedup, lang, PII)\par
  \Field{Epic / Feature:} Chapter 2 — Data Cleaning\par
  \Field{Business Value:} Improves training/eval quality; reduces safety risk and waste.\par
  \Field{Priority / Estimate:} P1, 5 SP\par
  \Field{Persona:} Data Engineer\par
  \Field{Dependencies:} Raw corpus; basic infra for ETL.\par
  \Field{Assumptions / Risks:} PII detection coverage; false positives.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As a data engineer, I want a cleaning pipeline with deduplication, language detection, and PII scrubbing so that downstream training/eval is trustworthy.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} an input corpus, \textbf{when} I run the pipeline, \textbf{then} duplicates are reduced by $\geq$ 95\% and a dedup report is generated.
    \item \textbf{Given} multilingual content, \textbf{when} language filtering is enabled, \textbf{then} non-target languages are removed with $\geq$ 98\% precision.
    \item \textbf{Given} PII patterns, \textbf{when} I scan and mask, \textbf{then} a PII audit log is produced with no unmasked samples in a 1k spot-check.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item \texttt{cleaning\_pipeline.ipynb}, configs; \texttt{cleaning\_report.md}.
    \item Sample before/after stats; CI job log.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch03 ----------
\begin{StoryCard}{CH03 — Data Augmentation}
  \Field{ID:} LLMDP-CH03-01\par
  \Field{Title:} Augment training data without semantic drift\par
  \Field{Epic / Feature:} Chapter 3 — Augmentation Patterns\par
  \Field{Business Value:} Improves coverage and robustness with minimal label cost.\par
  \Field{Priority / Estimate:} P2, 5 SP\par
  \Field{Persona:} ML Engineer\par
  \Field{Dependencies:} Clean dataset; augmentation tools.\par
  \Field{Assumptions / Risks:} Semantic drift; distribution shift.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an ML engineer, I want back-translation and paraphrase augmentation so that I increase coverage while avoiding meaning drift.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} 5k samples, \textbf{when} I augment, \textbf{then} embedding-similarity medians stay $\geq$ 0.9 vs. originals.
    \item \textbf{Given} baseline performance, \textbf{when} I train with augmented data, \textbf{then} held-out accuracy improves by $\geq$ 3\%.
    \item \textbf{Given} cost constraints, \textbf{when} I run augmentation, \textbf{then} I produce a cost/time report and data cards.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item \texttt{augment\_eval.md}, similarity histograms; MLflow runs.
    \item Data card JSON; PR with configs.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch04 ----------
\begin{StoryCard}{CH04 — Handling Large Datasets}
  \Field{ID:} LLMDP-CH04-01\par
  \Field{Title:} Streamable sharded dataset with throughput profiling\par
  \Field{Epic / Feature:} Chapter 4 — Data at Scale\par
  \Field{Business Value:} Enables fast training without memory bottlenecks.\par
  \Field{Priority / Estimate:} P2, 3 SP\par
  \Field{Persona:} ML Engineer\par
  \Field{Dependencies:} Object storage; parquet/arrow tooling.\par
  \Field{Assumptions / Risks:} I/O hotspots; shard imbalance.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an ML engineer, I want sharded parquet with a streaming loader so that I can train efficiently on large corpora.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} a 200GB corpus, \textbf{when} I stream batches, \textbf{then} GPU utilization stays $\geq$ 80\% on average.
    \item \textbf{Given} multiple workers, \textbf{when} I read shards, \textbf{then} imbalance is $\leq$ 10\% and no worker starves.
    \item \textbf{Given} profiles, \textbf{when} I run a 10-minute soak, \textbf{then} I/O throughput and p95 latency are reported.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Loader script; profiler screenshots; throughput CSV.
    \item README with tuning tips.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch05 ----------
\begin{StoryCard}{CH05 — Data Versioning}
  \Field{ID:} LLMDP-CH05-01\par
  \Field{Title:} Track dataset lineage with DVC and run linkage\par
  \Field{Epic / Feature:} Chapter 5 — Versioning Patterns\par
  \Field{Business Value:} Enables reproducibility and auditability.\par
  \Field{Priority / Estimate:} P1, 3 SP\par
  \Field{Persona:} MLOps Engineer\par
  \Field{Dependencies:} Git repo; object store; CI.\par
  \Field{Assumptions / Risks:} Storage costs; remote access.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an MLOps engineer, I want DVC-tracked datasets linked to experiment runs so that I can reproduce any model with exact data.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} dataset v1, \textbf{when} I run training, \textbf{then} MLflow stores the DVC hash with the run.
    \item \textbf{Given} dataset v2, \textbf{when} I diff, \textbf{then} I see added/removed records and risk notes.
    \item \textbf{Given} an audit, \textbf{when} I checkout a past run, \textbf{then} I can reproduce metrics within 1\%.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item \texttt{dvc.yaml}; MLflow link; lineage diagram.
    \item ADR documenting retention policy.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch06 ----------
\begin{StoryCard}{CH06 — Annotation \& Labeling}
  \Field{ID:} LLMDP-CH06-01\par
  \Field{Title:} Set up labeling guidelines with QA and IAA\par
  \Field{Epic / Feature:} Chapter 6 — Annotation Patterns\par
  \Field{Business Value:} Higher label quality; less noise.\par
  \Field{Priority / Estimate:} P2, 5 SP\par
  \Field{Persona:} Data PM / Label Lead\par
  \Field{Dependencies:} Labeling tool; sampling strategy.\par
  \Field{Assumptions / Risks:} Annotator drift; budget.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As a data PM, I want clear guidelines and QA sampling so that labels are consistent and reproducible.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} two annotators, \textbf{when} I compute agreement, \textbf{then} Cohen's $\kappa \geq 0.75$ on a 200-sample set.
    \item \textbf{Given} QA rules, \textbf{when} I review 10\% of labels, \textbf{then} correction rate is $< 5\%$ after iteration 2.
    \item \textbf{Given} guideline updates, \textbf{when} I re-run a small batch, \textbf{then} agreement improves vs. baseline.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item \texttt{label\_guidelines.md}; QA dashboard; IAA report.
    \item Example labeled JSONL before/after.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch07 ----------
\begin{StoryCard}{CH07 — Training Pipeline}
  \Field{ID:} LLMDP-CH07-01\par
  \Field{Title:} Config-driven modular training with retries\par
  \Field{Epic / Feature:} Chapter 7 — Training Pipeline\par
  \Field{Business Value:} Reproducible training; fewer failed runs.\par
  \Field{Priority / Estimate:} P1, 5 SP\par
  \Field{Persona:} ML Engineer\par
  \Field{Dependencies:} Hydra/Lightning or equivalent; MLflow.\par
  \Field{Assumptions / Risks:} Preemptible nodes; spot failures.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an ML engineer, I want a modular pipeline with resume/retry so that long runs survive infra hiccups and are repeatable from config.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} a training config, \textbf{when} I kill the job mid-epoch, \textbf{then} resume restarts within $\leq$ 2 minutes and continues deterministically.
    \item \textbf{Given} a failed run, \textbf{when} retry policy triggers, \textbf{then} the job restarts up to $N$ times and logs structured reasons.
    \item \textbf{Given} a new seed, \textbf{when} I re-run, \textbf{then} metrics vary within expected CIs.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item \texttt{config.yaml}; \texttt{pipeline/} code; MLflow runs.
    \item Failure-injection notes and logs.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch08 ----------
\begin{StoryCard}{CH08 — Hyperparameter Tuning}
  \Field{ID:} LLMDP-CH08-01\par
  \Field{Title:} Run budgeted HPO with early stopping\par
  \Field{Epic / Feature:} Chapter 8 — HPO Patterns\par
  \Field{Business Value:} Improves quality under cost constraints.\par
  \Field{Priority / Estimate:} P2, 5 SP\par
  \Field{Persona:} ML Engineer\par
  \Field{Dependencies:} Optuna/W\&B; search space defined.\par
  \Field{Assumptions / Risks:} Overfitting to val set; budget limits.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an ML engineer, I want random+Bayesian HPO with early stopping so that I improve accuracy without exceeding budget.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} a \$X budget, \textbf{when} I run 50 trials with pruning, \textbf{then} best validation metric improves $\geq$ 3\%.
    \item \textbf{Given} multi-objective metrics, \textbf{when} I analyze Pareto front, \textbf{then} I select a config with balanced cost/quality.
    \item \textbf{Given} a reproducibility check, \textbf{when} I re-run top-3, \textbf{then} results stay within 1\% variance.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item HPO report; plots; selected config.
    \item Cost breakdown and sweep artifacts.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch09 ----------
\begin{StoryCard}{CH09 — Regularization}
  \Field{ID:} LLMDP-CH09-01\par
  \Field{Title:} Reduce overfit via dropout, weight decay, clipping\par
  \Field{Epic / Feature:} Chapter 9 — Regularization Patterns\par
  \Field{Business Value:} Improves generalization and stability.\par
  \Field{Priority / Estimate:} P3, 3 SP\par
  \Field{Persona:} ML Engineer\par
  \Field{Dependencies:} Baseline model; eval suite.\par
  \Field{Assumptions / Risks:} Underfitting if overly aggressive.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an ML engineer, I want to tune regularization knobs so that the model generalizes better without hurting accuracy.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} a baseline, \textbf{when} I apply dropout/decay/clipping ablations, \textbf{then} held-out improves $\geq$ 2\% without latency penalty $>5\%$.
    \item \textbf{Given} instability, \textbf{when} I enable gradient clipping, \textbf{then} loss spikes disappear (plots attached).
    \item \textbf{Given} 3 seeds, \textbf{when} I evaluate, \textbf{then} dispersion narrows vs. baseline.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Ablation notebook; metrics table; training curves.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch10 ----------
\begin{StoryCard}{CH10 — Checkpointing \& Recovery}
  \Field{ID:} LLMDP-CH10-01\par
  \Field{Title:} Design checkpoint schedule with retention policy\par
  \Field{Epic / Feature:} Chapter 10 — Reliability Patterns\par
  \Field{Business Value:} Reduces lost compute; enables audit.\par
  \Field{Priority / Estimate:} P1, 3 SP\par
  \Field{Persona:} MLOps Engineer\par
  \Field{Dependencies:} Storage; scheduler; pipeline hooks.\par
  \Field{Assumptions / Risks:} Storage quotas; restore failures.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an MLOps engineer, I want structured checkpoints and restore tests so that long runs can be resumed safely.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} policy N\_keep=3, \textbf{when} training runs, \textbf{then} only the last 3 checkpoints remain and older ones are GC'd.
    \item \textbf{Given} failure injection, \textbf{when} I restore, \textbf{then} training resumes within 2 minutes and metrics match within 0.5\%.
    \item \textbf{Given} a compliance review, \textbf{when} I list artifacts, \textbf{then} hashes and metadata are present.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Checkpoint policy doc; restore logs; checksum table.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch11 ----------
\begin{StoryCard}{CH11 — Fine-Tuning}
  \Field{ID:} LLMDP-CH11-01\par
  \Field{Title:} Domain adaptation via LoRA/QLoRA baseline\par
  \Field{Epic / Feature:} Chapter 11 — Fine-Tuning Patterns\par
  \Field{Business Value:} Improves domain performance at lower cost.\par
  \Field{Priority / Estimate:} P1, 5 SP\par
  \Field{Persona:} ML Engineer\par
  \Field{Dependencies:} Base model; domain corpus; eval set.\par
  \Field{Assumptions / Risks:} Catastrophic forgetting risk.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an ML engineer, I want a LoRA/QLoRA fine-tune so that domain metrics improve with modest hardware.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} domain dataset, \textbf{when} I fine-tune, \textbf{then} task metric improves $\geq$ 5\% vs. SFT-free baseline.
    \item \textbf{Given} generic eval set, \textbf{when} I test, \textbf{then} no major regressions ($<2\%$ absolute).
    \item \textbf{Given} cost limits, \textbf{when} I log tokens and time, \textbf{then} run stays within budget.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Fine-tune notebook; config; before/after scores.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch12 ----------
\begin{StoryCard}{CH12 — Pruning}
  \Field{ID:} LLMDP-CH12-01\par
  \Field{Title:} Structured pruning for smaller, faster inference\par
  \Field{Epic / Feature:} Chapter 12 — Model Compression\par
  \Field{Business Value:} Cuts latency and memory without large quality loss.\par
  \Field{Priority / Estimate:} P3, 3 SP\par
  \Field{Persona:} ML Engineer\par
  \Field{Dependencies:} Baseline model; pruning toolkit.\par
  \Field{Assumptions / Risks:} Accuracy drop; hardware quirks.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an ML engineer, I want to prune 10--30\% of parameters so that I improve throughput with minimal loss.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} a pruned model, \textbf{when} I benchmark, \textbf{then} latency improves $\geq$ 20\% at p95 with $<2\%$ accuracy drop.
    \item \textbf{Given} traffic mix, \textbf{when} I load-test, \textbf{then} throughput increases proportionally and remains stable for 30 minutes.
    \item \textbf{Given} compatibility tests, \textbf{when} I deploy, \textbf{then} no unsupported ops surface.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Benchmark CSV; plots; pruning config.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch13 ----------
\begin{StoryCard}{CH13 — Quantization}
  \Field{ID:} LLMDP-CH13-01\par
  \Field{Title:} Compare 8-bit vs 4-bit quantization\par
  \Field{Epic / Feature:} Chapter 13 — Quantization Patterns\par
  \Field{Business Value:} Reduces memory; enables edge or CPU inference.\par
  \Field{Priority / Estimate:} P2, 4 SP\par
  \Field{Persona:} ML Engineer\par
  \Field{Dependencies:} AWQ/GPTQ/AutoGPTQ; eval harness.\par
  \Field{Assumptions / Risks:} Quality loss on long-form tasks.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an ML engineer, I want to evaluate 8-bit vs 4-bit to pick the best memory/quality trade-off for our serving stack.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} baseline FP16, \textbf{when} I quantize, \textbf{then} memory drops $\geq$ 50\% (8b) and $\geq$ 70\% (4b).
    \item \textbf{Given} task set, \textbf{when} I evaluate, \textbf{then} 8b quality loss $\leq$ 1\% and 4b $\leq$ 3\%.
    \item \textbf{Given} latency targets, \textbf{when} I measure p95, \textbf{then} latency improves $\geq$ 20\%.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Metrics table; hardware notes; deployment checklist.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch14 ----------
\begin{StoryCard}{CH14 — Evaluation Metrics}
  \Field{ID:} LLMDP-CH14-01\par
  \Field{Title:} Build task-aligned metrics and eval suite\par
  \Field{Epic / Feature:} Chapter 14 — Evaluation Patterns\par
  \Field{Business Value:} Honest progress measurement; regression detection.\par
  \Field{Priority / Estimate:} P1, 4 SP\par
  \Field{Persona:} QA Engineer\par
  \Field{Dependencies:} Datasets; metric scripts; dashboarding.\par
  \Field{Assumptions / Risks:} Metric gaming risk.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As a QA engineer, I want a standard eval suite so that we can compare models fairly across tasks and time.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} 3 core tasks, \textbf{when} I run the suite, \textbf{then} metrics are logged with CI status and trend charts.
    \item \textbf{Given} a new model, \textbf{when} I evaluate, \textbf{then} baseline deltas are computed with significance where possible.
    \item \textbf{Given} a regression, \textbf{when} thresholds trip, \textbf{then} CI fails with a link to diffs.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Eval scripts; dashboard URL; threshold policy doc.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch15 ----------
\begin{StoryCard}{CH15 — Cross-Validation}
  \Field{ID:} LLMDP-CH15-01\par
  \Field{Title:} K-fold protocol across domain slices\par
  \Field{Epic / Feature:} Chapter 15 — Generalization Patterns\par
  \Field{Business Value:} Reduces overfitting to narrow distributions.\par
  \Field{Priority / Estimate:} P2, 3 SP\par
  \Field{Persona:} QA Engineer\par
  \Field{Dependencies:} Dataset partitions; eval harness.\par
  \Field{Assumptions / Risks:} Leakage risk if split poorly.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As a QA engineer, I want k-fold CV by domain so that we estimate true generalization and spot overfit.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} four domains, \textbf{when} I split, \textbf{then} no sample appears in both train and test folds.
    \item \textbf{Given} CV runs, \textbf{when} I summarize, \textbf{then} mean$\pm$std is reported with confidence intervals.
    \item \textbf{Given} variability, \textbf{when} I analyze, \textbf{then} action items are filed for high-variance domains.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Split manifest; metrics report; leakage checks.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch16 ----------
\begin{StoryCard}{CH16 — Interpretability}
  \Field{ID:} LLMDP-CH16-01\par
  \Field{Title:} Run attribution/probing on selected tasks\par
  \Field{Epic / Feature:} Chapter 16 — Interpretability Patterns\par
  \Field{Business Value:} Increases trust and debugging speed.\par
  \Field{Priority / Estimate:} P3, 4 SP\par
  \Field{Persona:} Research Engineer\par
  \Field{Dependencies:} Probing tools; datasets.\par
  \Field{Assumptions / Risks:} Misinterpretation of probes.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As a research engineer, I want attribution and probes so that we can explain behaviors and target fixes.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} failing items, \textbf{when} I run attribution, \textbf{then} salient tokens and layers are identified with visuals.
    \item \textbf{Given} hypotheses, \textbf{when} I run probes, \textbf{then} results support/contradict with metrics.
    \item \textbf{Given} insights, \textbf{when} I propose changes, \textbf{then} follow-up tickets are created with expected impact.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Notebooks; plots; summary memo.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch17 ----------
\begin{StoryCard}{CH17 — Fairness \& Bias}
  \Field{ID:} LLMDP-CH17-01\par
  \Field{Title:} Detect and mitigate bias across subgroups\par
  \Field{Epic / Feature:} Chapter 17 — Fairness Patterns\par
  \Field{Business Value:} Reduces harm and compliance risk.\par
  \Field{Priority / Estimate:} P1, 5 SP\par
  \Field{Persona:} Responsible AI Lead\par
  \Field{Dependencies:} Fairness metrics; subgroup labels where lawful.\par
  \Field{Assumptions / Risks:} Sensitive data handling.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an RAI lead, I want subgroup tests and mitigations so that we reduce disparate performance across groups.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} subgroup data, \textbf{when} I evaluate, \textbf{then} gaps $>=$ 5\% are flagged with confidence.
    \item \textbf{Given} a mitigation, \textbf{when} I re-evaluate, \textbf{then} gap reduces by $\geq$ 50\% without global regression $>2\%$.
    \item \textbf{Given} governance, \textbf{when} I log, \textbf{then} review artifacts are archived per policy.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Fairness report; mitigation PR; governance ticket.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch18 ----------
\begin{StoryCard}{CH18 — Adversarial Robustness}
  \Field{ID:} LLMDP-CH18-01\par
  \Field{Title:} Build a prompt-attack harness and hardening plan\par
  \Field{Epic / Feature:} Chapter 18 — Robustness Patterns\par
  \Field{Business Value:} Protects against jailbreaks and abuse.\par
  \Field{Priority / Estimate:} P1, 6 SP\par
  \Field{Persona:} Security Engineer\par
  \Field{Dependencies:} Safety policies; red-team prompts.\par
  \Field{Assumptions / Risks:} Evolving threats.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As a security engineer, I want adversarial tests and mitigations so that the model resists common jailbreaks.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} a curated attack set, \textbf{when} I evaluate, \textbf{then} bypass rate is reported with severity tags.
    \item \textbf{Given} guardrails, \textbf{when} I harden, \textbf{then} bypass rate reduces by $\geq$ 50\% with minimal false positives.
    \item \textbf{Given} recurring tests, \textbf{when} CI runs weekly, \textbf{then} regressions are flagged.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Attack harness; reports; mitigation PRs.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch19 ----------
\begin{StoryCard}{CH19 — RLHF \& Preference Optimization}
  \Field{ID:} LLMDP-CH19-01\par
  \Field{Title:} Small-scale RM + PPO vs SFT baseline\par
  \Field{Epic / Feature:} Chapter 19 — Alignment Patterns\par
  \Field{Business Value:} Aligns outputs to user preferences.\par
  \Field{Priority / Estimate:} P2, 8 SP\par
  \Field{Persona:} Research Engineer\par
  \Field{Dependencies:} Preference data; compute; safety checks.\par
  \Field{Assumptions / Risks:} Instability; reward hacking.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As a research engineer, I want a small reward model and PPO fine-tune so that outputs better match preferences.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} preference pairs, \textbf{when} I train RM, \textbf{then} validation accuracy $\geq$ 65\%.
    \item \textbf{Given} PPO, \textbf{when} I evaluate, \textbf{then} user-preference win rate improves $\geq$ 10\% vs. SFT-only.
    \item \textbf{Given} safety checks, \textbf{when} I run tests, \textbf{then} toxic/unsafe rate does not increase.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item RM/ PPO configs; win-rate report; safety logs.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch20 ----------
\begin{StoryCard}{CH20 — Chain-of-Thought Prompting}
  \Field{ID:} LLMDP-CH20-01\par
  \Field{Title:} Add CoT templates with cost/latency controls\par
  \Field{Epic / Feature:} Chapter 20 — Reasoning Patterns\par
  \Field{Business Value:} Improves step-by-step reasoning quality.\par
  \Field{Priority / Estimate:} P2, 4 SP\par
  \Field{Persona:} Prompt Engineer\par
  \Field{Dependencies:} Evaluation harness; sampling control.\par
  \Field{Assumptions / Risks:} Token bloat; leakage of traces.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As a prompt engineer, I want CoT templates and self-consistency so that hard tasks are solved more reliably.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} a reasoning set, \textbf{when} I enable CoT, \textbf{then} accuracy improves $\geq$ 8\% with cost reported.
    \item \textbf{Given} k-sampling, \textbf{when} I sweep k, \textbf{then} I select a setting within latency SLO.
    \item \textbf{Given} logs, \textbf{when} I store traces, \textbf{then} PII and secrets are redacted.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Template library; results table; cost analysis.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch21 ----------
\begin{StoryCard}{CH21 — Tree-of-Thoughts}
  \Field{ID:} LLMDP-CH21-01\par
  \Field{Title:} Implement beam search ToT with pruning heuristics\par
  \Field{Epic / Feature:} Chapter 21 — Structured Reasoning\par
  \Field{Business Value:} Explores solution paths and reduces dead-ends.\par
  \Field{Priority / Estimate:} P3, 5 SP\par
  \Field{Persona:} Prompt Engineer\par
  \Field{Dependencies:} Search framework; eval tasks.\par
  \Field{Assumptions / Risks:} Higher cost; complexity.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As a prompt engineer, I want ToT with pruning so that complex tasks benefit from structured exploration.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} a puzzle set, \textbf{when} I enable ToT, \textbf{then} accuracy improves vs. CoT-only with cost tracked.
    \item \textbf{Given} pruning, \textbf{when} I tune thresholds, \textbf{then} expansions drop $\geq$ 40\% with minimal accuracy loss.
    \item \textbf{Given} failures, \textbf{when} I analyze reasons, \textbf{then} I file tuning actions.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item ToT notebook; search stats; comparison plots.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch22 ----------
\begin{StoryCard}{CH22 — ReAct (Reason+Act with Tools)}
  \Field{ID:} LLMDP-CH22-01\par
  \Field{Title:} Build a ReAct agent using search + calculator tools\par
  \Field{Epic / Feature:} Chapter 22 — Tool-Augmented Reasoning\par
  \Field{Business Value:} Enables grounded answers and calculations.\par
  \Field{Priority / Estimate:} P2, 6 SP\par
  \Field{Persona:} Agent Engineer\par
  \Field{Dependencies:} Tool schemas; sandbox.\par
  \Field{Assumptions / Risks:} Tool failures; injection attacks.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an agent engineer, I want a ReAct loop with two tools so that the agent can plan, cite, and compute.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} tool specs, \textbf{when} I run tasks, \textbf{then} the agent performs plan $\rightarrow$ act traces with citations.
    \item \textbf{Given} tool errors, \textbf{when} a call fails, \textbf{then} the agent retries or falls back gracefully.
    \item \textbf{Given} prompt injection tests, \textbf{when} I evaluate, \textbf{then} the agent declines unsafe tool calls.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Agent code; transcripts; red-team report.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch23 ----------
\begin{StoryCard}{CH23 — ReWOO (Plan-then-Act)}
  \Field{ID:} LLMDP-CH23-01\par
  \Field{Title:} Implement ReWOO and compare to ReAct\par
  \Field{Epic / Feature:} Chapter 23 — Planning Patterns\par
  \Field{Business Value:} Reduces tool thrashing and errors.\par
  \Field{Priority / Estimate:} P3, 5 SP\par
  \Field{Persona:} Agent Engineer\par
  \Field{Dependencies:} Planner; tool executor.\par
  \Field{Assumptions / Risks:} Longer initial planning; drift risk.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an agent engineer, I want a ReWOO pipeline so that the agent plans before calling tools and we can compare with ReAct.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} a multi-hop task, \textbf{when} I run ReWOO, \textbf{then} the plan is explicit and tool calls follow the plan.
    \item \textbf{Given} the same tasks, \textbf{when} I compare with ReAct, \textbf{then} tool calls decrease by $\geq$ 20\% with equal or better accuracy.
    \item \textbf{Given} failure cases, \textbf{when} I analyze, \textbf{then} I identify plan quality issues and fixes.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Plans and traces; comparison report; tuning notes.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch24 ----------
\begin{StoryCard}{CH24 — Reflection Techniques}
  \Field{ID:} LLMDP-CH24-01\par
  \Field{Title:} Add critique-and-retry loop to failing tasks\par
  \Field{Epic / Feature:} Chapter 24 — Self-Improvement Patterns\par
  \Field{Business Value:} Increases pass rate on hard cases.\par
  \Field{Priority / Estimate:} P2, 3 SP\par
  \Field{Persona:} Prompt Engineer\par
  \Field{Dependencies:} Eval harness; error taxonomy.\par
  \Field{Assumptions / Risks:} Overfitting to eval set.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As a prompt engineer, I want a reflection step so that the model self-critiques and retries on hard tasks.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} failing items, \textbf{when} I enable reflection, \textbf{then} pass rate improves $\geq$ 5\% with cost logged.
    \item \textbf{Given} risk of verbosity, \textbf{when} I cap tokens, \textbf{then} cost stays within budget.
    \item \textbf{Given} logs, \textbf{when} I analyze, \textbf{then} common failure modes are categorized.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Reflection prompts; before/after metrics; cost sheet.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch25 ----------
\begin{StoryCard}{CH25 — Automatic Multi-Step Reasoning \& Tools}
  \Field{ID:} LLMDP-CH25-01\par
  \Field{Title:} Build a task graph with tool selection heuristics\par
  \Field{Epic / Feature:} Chapter 25 — Multi-Step Patterns\par
  \Field{Business Value:} Autonomously decomposes tasks and selects tools.\par
  \Field{Priority / Estimate:} P3, 6 SP\par
  \Field{Persona:} Agent Engineer\par
  \Field{Dependencies:} Graph framework; tools registry.\par
  \Field{Assumptions / Risks:} Loops; dead-ends.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an agent engineer, I want a task graph with heuristics so that the agent executes multi-step tasks reliably.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} composite tasks, \textbf{when} I run the graph, \textbf{then} steps execute with retries and backoff.
    \item \textbf{Given} tool registry, \textbf{when} I select tools, \textbf{then} correct tools are chosen $\geq$ 90\% on a labeled set.
    \item \textbf{Given} loops, \textbf{when} I enforce limits, \textbf{then} execution stops with a clear error.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Graph JSON; run logs; evaluation sheet.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch26 ----------
\begin{StoryCard}{CH26 — Retrieval-Augmented Generation (RAG)}
  \Field{ID:} LLMDP-CH26-02\par
  \Field{Title:} Productionize RAG with ingestion + query APIs\par
  \Field{Epic / Feature:} Chapter 26 — RAG Patterns\par
  \Field{Business Value:} Grounded answers; lower hallucination.\par
  \Field{Priority / Estimate:} P1, 8 SP\par
  \Field{Persona:} Platform Engineer\par
  \Field{Dependencies:} Vector DB; embedding model; store.\par
  \Field{Assumptions / Risks:} Stale indexes; drift.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As a platform engineer, I want ingestion and query APIs for RAG so that apps can use grounded answers at scale.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} ingestion, \textbf{when} new docs arrive, \textbf{then} they are chunked, embedded, and indexed within SLA.
    \item \textbf{Given} queries, \textbf{when} I pass a question, \textbf{then} top-$k$ docs and citations are returned with latency SLO.
    \item \textbf{Given} drift, \textbf{when} I re-index, \textbf{then} recall improves on a held-out query set.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item API spec; load test; eval metrics (retrieval/generation).
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch27 ----------
\begin{StoryCard}{CH27 — Graph-Based RAG}
  \Field{ID:} LLMDP-CH27-01\par
  \Field{Title:} Build a small knowledge graph and hybrid retrieval\par
  \Field{Epic / Feature:} Chapter 27 — Graph RAG Patterns\par
  \Field{Business Value:} Improves retrieval on relational queries.\par
  \Field{Priority / Estimate:} P2, 6 SP\par
  \Field{Persona:} Data Engineer\par
  \Field{Dependencies:} KG store; ETL from docs.\par
  \Field{Assumptions / Risks:} Graph build cost; schema drift.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As a data engineer, I want entity/relation extraction and graph queries so that retrieval exploits structure and semantics.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} a corpus, \textbf{when} I extract entities/relations, \textbf{then} KG is populated with precision $\geq$ 0.9 on a checked sample.
    \item \textbf{Given} hybrid retrieval, \textbf{when} I compare to dense-only, \textbf{then} hit-rate improves on relational questions.
    \item \textbf{Given} updates, \textbf{when} I rebuild edges, \textbf{then} graph stays consistent with audit logs.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item KG schema; extraction notebook; hybrid eval report.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch28 ----------
\begin{StoryCard}{CH28 — Advanced RAG (Query Reformulation \& Re-Ranking)}
  \Field{ID:} LLMDP-CH28-01\par
  \Field{Title:} Add re-ranking and iterative query reformulation\par
  \Field{Epic / Feature:} Chapter 28 — Advanced RAG Patterns\par
  \Field{Business Value:} Boosts answer quality on ambiguous queries.\par
  \Field{Priority / Estimate:} P3, 5 SP\par
  \Field{Persona:} LLM Engineer\par
  \Field{Dependencies:} Reranker; query strategies.\par
  \Field{Assumptions / Risks:} Extra latency; complexity.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an LLM engineer, I want iterative reformulation and re-ranking so that difficult queries retrieve better evidence.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} ambiguous questions, \textbf{when} I enable reformulation, \textbf{then} answer correctness improves vs. baseline.
    \item \textbf{Given} a reranker, \textbf{when} I insert it, \textbf{then} nDCG@k increases with bounded latency overhead.
    \item \textbf{Given} tail queries, \textbf{when} I evaluate, \textbf{then} failure rate decreases by $\geq$ 15\%.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Reformulation prompts; reranker config; metrics.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch29 ----------
\begin{StoryCard}{CH29 — Evaluating RAG Systems}
  \Field{ID:} LLMDP-CH29-01\par
  \Field{Title:} Build an end-to-end RAG evaluation scorecard\par
  \Field{Epic / Feature:} Chapter 29 — RAG Evaluation Patterns\par
  \Field{Business Value:} Detects regressions across retrieval and generation.\par
  \Field{Priority / Estimate:} P1, 4 SP\par
  \Field{Persona:} QA Engineer\par
  \Field{Dependencies:} Ground-truth Q/A; judgments.\par
  \Field{Assumptions / Risks:} Labeling cost.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As a QA engineer, I want a RAG scorecard so that retrieval and answer metrics are tracked together over time.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} a test set, \textbf{when} I run the scorecard, \textbf{then} retrieval (recall, nDCG) and generation (factuality, faithfulness) are produced.
    \item \textbf{Given} a new release, \textbf{when} I compare, \textbf{then} deltas are highlighted and gates enforced.
    \item \textbf{Given} monitoring, \textbf{when} I sample prod queries, \textbf{then} drift alerts trigger when KPIs degrade.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Scorecard notebook; dashboard; gating policy.
  \end{TasksBox}
\end{StoryCard}

% ---------- Ch30 ----------
\begin{StoryCard}{CH30 — Agentic Patterns (Planning, Memory, Safety)}
  \Field{ID:} LLMDP-CH30-01\par
  \Field{Title:} Ship a single-agent demo with short-term memory and guardrails\par
  \Field{Epic / Feature:} Chapter 30 — Agentic Patterns\par
  \Field{Business Value:} Demonstrates plan/act with risk controls.\par
  \Field{Priority / Estimate:} P2, 8 SP\par
  \Field{Persona:} Agent Engineer\par
  \Field{Dependencies:} Memory store; tools; policy filters.\par
  \Field{Assumptions / Risks:} Tool abuse; privacy constraints.\par
  \medskip
  \begin{TasksBox}[User Story]
    \item As an agent engineer, I want an agent with planning, memory, and guardrails so that it can safely complete tasks with tools.
  \end{TasksBox}
  \begin{TasksBox}[Acceptance Criteria (BDD)]
    \item \textbf{Given} a goal, \textbf{when} I run the agent, \textbf{then} it creates a plan, executes tools, and stores relevant memory.
    \item \textbf{Given} safety policies, \textbf{when} unsafe requests occur, \textbf{then} the agent refuses and cites policy.
    \item \textbf{Given} eval tasks, \textbf{when} I measure, \textbf{then} success rate $\geq$ target with incident rate $< 1\%$.
  \end{TasksBox}
  \begin{TasksBox}[Evidence to Attach]
    \item Agent traces; memory snapshots; safety report.
  \end{TasksBox}
\end{StoryCard}

\end{document}