\documentclass[11pt,a4paper]{report}

% ============================================================
% PACKAGES
% ============================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{tcolorbox}
\usepackage{tikz}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{fontawesome5}
\usepackage{mdframed}
\usepackage{etoolbox}
\usepackage{calc}
\usepackage{pgfplots}
\usepackage{adjustbox}
\usepackage{colortbl}
\usepackage{amssymb}
\usetikzlibrary{shapes.geometric, arrows, positioning, calc, fit, backgrounds, decorations.pathreplacing}
\pgfplotsset{compat=1.18}

% ============================================================
% COLOR DEFINITIONS
% ============================================================
\definecolor{primaryblue}{RGB}{25, 55, 95}
\definecolor{accentblue}{RGB}{52, 152, 219}
\definecolor{darkgray}{RGB}{45, 52, 54}
\definecolor{lightgray}{RGB}{245, 246, 250}
\definecolor{successgreen}{RGB}{39, 174, 96}
\definecolor{warningorange}{RGB}{243, 156, 18}
\definecolor{dangerred}{RGB}{231, 76, 60}
\definecolor{phase1color}{RGB}{155, 89, 182}
\definecolor{phase2color}{RGB}{52, 152, 219}
\definecolor{phase3color}{RGB}{46, 204, 113}
\definecolor{phase4color}{RGB}{241, 196, 15}
\definecolor{foundationcolor}{RGB}{142, 68, 173}
\definecolor{intermediatecolor}{RGB}{41, 128, 185}
\definecolor{advancedcolor}{RGB}{39, 174, 96}
\definecolor{expertcolor}{RGB}{243, 156, 18}
\definecolor{trackA}{RGB}{230, 126, 34}
\definecolor{trackB}{RGB}{52, 73, 94}
\definecolor{trackC}{RGB}{22, 160, 133}

% ============================================================
% HYPERREF CONFIGURATION
% ============================================================
\hypersetup{
    colorlinks=true,
    linkcolor=primaryblue,
    filecolor=accentblue,
    urlcolor=accentblue,
    citecolor=primaryblue,
    pdftitle={Prompt Engineering and LLM Application Development Program Roadmap},
    pdfauthor={},
    pdfsubject={Comprehensive Learning Roadmap},
    pdfkeywords={prompt engineering, LLM, AI, machine learning, roadmap, curriculum}
}

% ============================================================
% PAGE STYLE CONFIGURATION
% ============================================================
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\textcolor{darkgray}{\leftmark}}
\fancyhead[R]{\small\textcolor{darkgray}{PE \& LLM Development Roadmap}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% ============================================================
% TITLE FORMATTING
% ============================================================
\titleformat{\chapter}[display]
{\normalfont\huge\bfseries\color{primaryblue}}
{\chaptertitlename\ \thechapter}{20pt}{\Huge}

\titleformat{\section}
{\normalfont\Large\bfseries\color{primaryblue}}
{\thesection}{1em}{}

\titleformat{\subsection}
{\normalfont\large\bfseries\color{darkgray}}
{\thesubsection}{1em}{}

\titleformat{\subsubsection}
{\normalfont\normalsize\bfseries\color{darkgray}}
{\thesubsubsection}{1em}{}

% ============================================================
% CUSTOM ENVIRONMENTS
% ============================================================
\newtcolorbox{roadmapbox}[1][]{
    colback=lightgray,
    colframe=primaryblue,
    fonttitle=\bfseries,
    title={#1},
    arc=3mm,
    boxrule=1pt
}

\newtcolorbox{milestonebox}{
    colback=lightgray,
    colframe=successgreen,
    fonttitle=\bfseries,
    title={\faIcon{flag-checkered} Milestone},
    arc=3mm,
    boxrule=1pt
}

\newtcolorbox{skillbox}{
    colback=lightgray,
    colframe=accentblue,
    fonttitle=\bfseries,
    title={\faIcon{cogs} Skills Acquired},
    arc=3mm,
    boxrule=1pt
}

\newtcolorbox{prerequisitebox}{
    colback=lightgray,
    colframe=warningorange,
    fonttitle=\bfseries,
    title={\faIcon{exclamation-triangle} Prerequisites},
    arc=3mm,
    boxrule=1pt
}

\newtcolorbox{assessmentbox}{
    colback=lightgray,
    colframe=primaryblue,
    fonttitle=\bfseries,
    title={\faIcon{clipboard-check} Assessment Checkpoint},
    arc=3mm,
    boxrule=1pt
}

\newtcolorbox{weekbox}[1][]{
    colback=white,
    colframe=darkgray,
    fonttitle=\bfseries,
    title={#1},
    arc=2mm,
    boxrule=0.5pt
}

\newtcolorbox{trackbox}[2][]{
    colback=#2!5,
    colframe=#2,
    fonttitle=\bfseries,
    title={#1},
    arc=3mm,
    boxrule=1pt
}

\newtcolorbox{competencybox}[1][]{
    colback=lightgray,
    colframe=foundationcolor,
    fonttitle=\bfseries,
    title={\faIcon{graduation-cap} #1},
    arc=3mm,
    boxrule=1pt
}

\newtcolorbox{selfevalbox}{
    colback=lightgray,
    colframe=phase1color,
    fonttitle=\bfseries,
    title={\faIcon{tasks} Self-Evaluation Checklist},
    arc=3mm,
    boxrule=1pt
}

\newtcolorbox{projectbox}{
    colback=lightgray,
    colframe=phase3color,
    fonttitle=\bfseries,
    title={\faIcon{project-diagram} Project Deliverable},
    arc=3mm,
    boxrule=1pt
}

\newtcolorbox{timebox}{
    colback=lightgray,
    colframe=darkgray,
    fonttitle=\bfseries,
    title={\faIcon{clock} Time Investment},
    arc=3mm,
    boxrule=1pt
}

\newtcolorbox{tipbox}{
    colback=accentblue!10,
    colframe=accentblue,
    fonttitle=\bfseries,
    title={\faIcon{lightbulb} Pro Tip},
    arc=3mm,
    boxrule=1pt
}

\newtcolorbox{warningbox}{
    colback=dangerred!10,
    colframe=dangerred,
    fonttitle=\bfseries,
    title={\faIcon{exclamation-circle} Common Pitfall},
    arc=3mm,
    boxrule=1pt
}

% ============================================================
% CUSTOM COMMANDS
% ============================================================
\newcommand{\levelindicator}[1]{%
    \ifcase#1
        \textcolor{foundationcolor}{\faIcon{star}}\textcolor{lightgray}{\faIcon{star}\faIcon{star}\faIcon{star}}%
    \or
        \textcolor{foundationcolor}{\faIcon{star}\faIcon{star}}\textcolor{lightgray}{\faIcon{star}\faIcon{star}}%
    \or
        \textcolor{intermediatecolor}{\faIcon{star}\faIcon{star}\faIcon{star}}\textcolor{lightgray}{\faIcon{star}}%
    \or
        \textcolor{advancedcolor}{\faIcon{star}\faIcon{star}\faIcon{star}\faIcon{star}}%
    \fi
}

\newcommand{\checkmark}{\textcolor{successgreen}{\ding{51}}}
\newcommand{\crossmark}{\textcolor{dangerred}{\ding{55}}}

% ============================================================
% DOCUMENT BEGIN
% ============================================================
\begin{document}

% ============================================================
% TITLE PAGE
% ============================================================
\begin{titlepage}
\begin{center}

\vspace*{0.5cm}

\begin{tikzpicture}
\fill[primaryblue] (0,0) rectangle (16,2.5);
\node[white, font=\Huge\bfseries, align=center] at (8,1.25) {COMPREHENSIVE PROGRAM ROADMAP};
\end{tikzpicture}

\vspace{1.5cm}

{\Huge\bfseries\textcolor{primaryblue}{Prompt Engineering \&\\[0.3cm] LLM Application Development}}

\vspace{0.8cm}

\begin{tikzpicture}
\draw[accentblue, line width=2pt] (0,0) -- (12,0);
\end{tikzpicture}

\vspace{0.8cm}

{\Large\textcolor{darkgray}{A Structured Learning Journey from\\Foundation to Expert-Level Mastery}}

\vspace{0.5cm}

{\large\textcolor{darkgray}{\textit{Technology Agnostic • Provider Independent • Industry Aligned}}}

\vspace{1.5cm}

% Visual Learning Path
\begin{tikzpicture}[
    node distance=0.8cm,
    phase/.style={draw, rounded corners, minimum width=2.8cm, minimum height=1.4cm, align=center, font=\small\bfseries},
    arrow/.style={->, line width=1.5pt, accentblue}
]
\node[phase, fill=phase1color!15, draw=phase1color] (p1) {\textcolor{phase1color}{Phase 1}\\Foundation\\(4 weeks)};
\node[phase, fill=phase2color!15, draw=phase2color, right=of p1] (p2) {\textcolor{phase2color}{Phase 2}\\Application\\(6 weeks)};
\node[phase, fill=phase3color!15, draw=phase3color, right=of p2] (p3) {\textcolor{phase3color}{Phase 3}\\Production\\(4 weeks)};
\node[phase, fill=phase4color!15, draw=phase4color, right=of p3] (p4) {\textcolor{phase4color}{Phase 4}\\Mastery\\(Ongoing)};

\draw[arrow] (p1) -- (p2);
\draw[arrow] (p2) -- (p3);
\draw[arrow] (p3) -- (p4);
\end{tikzpicture}

\vspace{1.5cm}

% Key Statistics
\begin{tikzpicture}
\node[draw=darkgray, rounded corners, minimum width=3cm, minimum height=1.2cm, align=center] at (0,0) {\textbf{14--18 Weeks}\\Core Program};
\node[draw=darkgray, rounded corners, minimum width=3cm, minimum height=1.2cm, align=center] at (4.5,0) {\textbf{200+ Hours}\\Learning Time};
\node[draw=darkgray, rounded corners, minimum width=3cm, minimum height=1.2cm, align=center] at (9,0) {\textbf{4 Capstones}\\Major Projects};
\node[draw=darkgray, rounded corners, minimum width=3cm, minimum height=1.2cm, align=center] at (13.5,0) {\textbf{50+ Skills}\\Competencies};
\end{tikzpicture}

\vfill

\begin{tikzpicture}
\fill[lightgray] (0,0) rectangle (16,1.5);
\node[darkgray, font=\small] at (8,0.75) {Version 2.0 | \today};
\end{tikzpicture}

\end{center}
\end{titlepage}

% ============================================================
% TABLE OF CONTENTS
% ============================================================
\tableofcontents
\newpage

% ============================================================
% PART I: PROGRAM OVERVIEW
% ============================================================
\part{Program Overview and Navigation}

% ============================================================
% CHAPTER: EXECUTIVE ROADMAP SUMMARY
% ============================================================
\chapter{Executive Roadmap Summary}
\label{ch:executive}

\section{Program Vision}

This comprehensive roadmap provides a structured, technology-agnostic pathway for mastering prompt engineering and building production-ready LLM-powered applications. The program is designed around the principle that effective LLM engineering requires a combination of foundational prompt crafting skills, software engineering best practices, and operational excellence.

The roadmap emphasizes practical application over theoretical knowledge, with each phase building upon the previous one through hands-on projects, iterative skill development, and progressive complexity. Learners emerge not just as prompt engineers, but as complete AI application developers capable of designing, building, deploying, and maintaining sophisticated LLM systems.

\section{Program Structure Overview}

\begin{center}
\begin{tikzpicture}[
    scale=0.9,
    phase/.style={draw, rounded corners, minimum width=3.5cm, minimum height=2cm, align=center},
    module/.style={draw, rounded corners, minimum width=2.8cm, minimum height=0.8cm, align=center, font=\scriptsize},
    arrow/.style={->, thick}
]

% Phase 1
\node[phase, fill=phase1color!20, draw=phase1color, line width=1.5pt] (p1) at (0,0) {
    \textbf{\textcolor{phase1color}{Phase 1}}\\
    \textbf{Foundation}\\
    \scriptsize 4 weeks
};

\node[module, fill=white] at (0,-2) {Prompting Fundamentals};
\node[module, fill=white] at (0,-3) {Advanced Patterns};
\node[module, fill=white] at (0,-4) {Evaluation Methods};
\node[module, fill=white] at (0,-5) {Specialized Domains};

% Phase 2
\node[phase, fill=phase2color!20, draw=phase2color, line width=1.5pt] (p2) at (5,0) {
    \textbf{\textcolor{phase2color}{Phase 2}}\\
    \textbf{Application}\\
    \scriptsize 6 weeks
};

\node[module, fill=white] at (5,-2) {Architecture Patterns};
\node[module, fill=white] at (5,-3) {Tool Integration};
\node[module, fill=white] at (5,-4) {RAG Systems};
\node[module, fill=white] at (5,-5) {Conversational AI};

% Phase 3
\node[phase, fill=phase3color!20, draw=phase3color, line width=1.5pt] (p3) at (10,0) {
    \textbf{\textcolor{phase3color}{Phase 3}}\\
    \textbf{Production}\\
    \scriptsize 4 weeks
};

\node[module, fill=white] at (10,-2) {System Design};
\node[module, fill=white] at (10,-3) {Evaluation at Scale};
\node[module, fill=white] at (10,-4) {LLMOps};
\node[module, fill=white] at (10,-5) {Safety \& Security};

% Phase 4
\node[phase, fill=phase4color!20, draw=phase4color, line width=1.5pt] (p4) at (15,0) {
    \textbf{\textcolor{phase4color}{Phase 4}}\\
    \textbf{Mastery}\\
    \scriptsize Ongoing
};

\node[module, fill=white] at (15,-2) {Advanced Techniques};
\node[module, fill=white] at (15,-3) {Research Frontiers};
\node[module, fill=white] at (15,-4) {Specialization};
\node[module, fill=white] at (15,-5) {Leadership};

% Arrows
\draw[arrow, accentblue, line width=2pt] (p1) -- (p2);
\draw[arrow, accentblue, line width=2pt] (p2) -- (p3);
\draw[arrow, accentblue, line width=2pt] (p3) -- (p4);

\end{tikzpicture}
\end{center}

\section{Learning Outcomes by Phase}

\subsection{Phase 1: Foundation (Weeks 1--4)}

Upon completing Phase 1, learners will demonstrate mastery of core prompting principles applicable across any LLM platform:

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{Prompt Construction:} Design clear, effective prompts that consistently produce high-quality outputs regardless of the underlying model
    \item \textbf{Pattern Recognition:} Identify and apply 25+ distinct prompting patterns to appropriate use cases
    \item \textbf{Reasoning Enhancement:} Implement chain-of-thought, self-consistency, and other reasoning amplification techniques
    \item \textbf{Quality Evaluation:} Systematically assess and iterate on prompt performance using both qualitative and quantitative methods
    \item \textbf{Multi-Modal Competence:} Extend prompting skills to code generation, structured outputs, and vision-language tasks
\end{itemize}

\subsection{Phase 2: Application Development (Weeks 5--10)}

Phase 2 transforms prompt engineers into application developers:

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{Architecture Design:} Select and implement appropriate architectural patterns for LLM applications
    \item \textbf{Tool Integration:} Enable LLMs to interact with external systems through function calling and tool use
    \item \textbf{Knowledge Augmentation:} Build retrieval-augmented generation (RAG) systems for knowledge-intensive applications
    \item \textbf{Conversational Systems:} Design stateful agents with memory, context management, and persona consistency
    \item \textbf{Provider Abstraction:} Create applications that work across multiple LLM providers without code changes
\end{itemize}

\subsection{Phase 3: Production Engineering (Weeks 11--14)}

Phase 3 focuses on operational excellence and production readiness:

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{System Design:} Make informed trade-offs between prompting, fine-tuning, and RAG approaches
    \item \textbf{Quality Assurance:} Implement comprehensive evaluation pipelines with automated and human-in-the-loop assessment
    \item \textbf{Observability:} Deploy monitoring, logging, and alerting systems for LLM applications
    \item \textbf{Cost Optimization:} Manage inference costs through caching, model selection, and prompt optimization
    \item \textbf{Security Hardening:} Protect against prompt injection, data leakage, and other LLM-specific vulnerabilities
\end{itemize}

\subsection{Phase 4: Mastery (Ongoing)}

Phase 4 represents continuous professional growth:

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{Advanced Techniques:} Master emerging patterns like constitutional AI, recursive self-improvement, and multi-agent orchestration
    \item \textbf{Research Integration:} Stay current with academic developments and integrate new techniques into practice
    \item \textbf{Domain Specialization:} Develop deep expertise in specific verticals (healthcare, finance, legal, etc.)
    \item \textbf{Technical Leadership:} Guide teams and organizations in LLM adoption and best practices
\end{itemize}

\section{Target Audience Profiles}

This roadmap accommodates multiple learner profiles with tailored pathways:

\begin{center}
\begin{tabularx}{\textwidth}{>{\bfseries}p{3.5cm}Xp{3cm}}
\toprule
\textbf{Profile} & \textbf{Background \& Goals} & \textbf{Recommended Track} \\
\midrule
Software Engineer & Experienced developer seeking to integrate LLM capabilities into existing applications & Standard (14 weeks) \\
\midrule
Data Scientist & ML practitioner transitioning to LLM-focused roles; familiar with model evaluation & Accelerated (10 weeks) \\
\midrule
Product Manager & Technical PM requiring deep understanding of LLM capabilities and limitations & Foundation+ (8 weeks) \\
\midrule
Technical Writer & Content creator using generative AI for documentation and communication & Custom (Phase 1 + select modules) \\
\midrule
Career Changer & Professional from another field entering AI/ML; strong analytical skills & Extended (18+ weeks) \\
\bottomrule
\end{tabularx}
\end{center}

% ============================================================
% CHAPTER: PREREQUISITES AND PREPARATION
% ============================================================
\chapter{Prerequisites and Preparation}
\label{ch:prerequisites}

\section{Required Knowledge Assessment}

Before beginning the program, learners should honestly assess their current competencies against the following requirements. This self-assessment determines your recommended starting point and pace.

\begin{prerequisitebox}
\textbf{Essential Prerequisites (Must Have):}
\begin{itemize}
    \item Proficiency in at least one programming language (Python strongly recommended)
    \item Understanding of basic software development concepts (version control, APIs, data structures)
    \item Familiarity with command-line interfaces and development environments
    \item Access to API credits from at least one major LLM provider
    \item Dedicated study time of 8--15 hours per week
\end{itemize}
\end{prerequisitebox}

\subsection{Technical Skills Matrix}

Rate your current proficiency (1 = Beginner, 5 = Expert) for each skill:

\begin{center}
\begin{tabularx}{\textwidth}{Xcccccc}
\toprule
\textbf{Skill Area} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{Required} \\
\midrule
Python programming & $\square$ & $\square$ & $\square$ & $\square$ & $\square$ & 3+ \\
REST API consumption & $\square$ & $\square$ & $\square$ & $\square$ & $\square$ & 2+ \\
JSON/YAML data formats & $\square$ & $\square$ & $\square$ & $\square$ & $\square$ & 3+ \\
Git version control & $\square$ & $\square$ & $\square$ & $\square$ & $\square$ & 2+ \\
Database fundamentals (SQL) & $\square$ & $\square$ & $\square$ & $\square$ & $\square$ & 2+ \\
Web development basics & $\square$ & $\square$ & $\square$ & $\square$ & $\square$ & 1+ \\
Machine learning concepts & $\square$ & $\square$ & $\square$ & $\square$ & $\square$ & 1+ \\
Cloud services familiarity & $\square$ & $\square$ & $\square$ & $\square$ & $\square$ & 1+ \\
\bottomrule
\end{tabularx}
\end{center}

\subsection{Prerequisite Gap Remediation}

If you identified gaps in the essential prerequisites, the following resources can help you prepare:

\begin{longtable}{p{4cm}p{5cm}p{5cm}}
\toprule
\textbf{Gap Area} & \textbf{Recommended Resource} & \textbf{Time Investment} \\
\midrule
\endhead
Python Programming & Python official tutorial; Codecademy Python course & 20--40 hours \\
\midrule
REST APIs & RESTful API tutorials; Postman learning center & 5--10 hours \\
\midrule
Git/Version Control & Pro Git book (free online); GitHub Learning Lab & 5--10 hours \\
\midrule
Command Line & Linux Command Line Basics; terminal tutorials & 5--8 hours \\
\midrule
JSON/Data Formats & JSON/YAML tutorials; practice with real APIs & 2--4 hours \\
\midrule
ML Concepts & 3Blue1Brown neural networks; fast.ai intro & 10--15 hours \\
\bottomrule
\end{longtable}

\section{Environment Setup Checklist}

Complete this checklist before beginning Week 1:

\begin{selfevalbox}
\textbf{Development Environment:}
\begin{itemize}
    \item[$\square$] Python 3.9+ installed with pip/conda
    \item[$\square$] Code editor or IDE configured (VS Code, PyCharm, etc.)
    \item[$\square$] Git installed and GitHub account created
    \item[$\square$] Virtual environment manager available (venv, conda, poetry)
    \item[$\square$] Jupyter Notebook or JupyterLab installed
\end{itemize}

\textbf{API Access:}
\begin{itemize}
    \item[$\square$] Primary LLM provider account and API key (any major provider)
    \item[$\square$] Secondary provider account (recommended for comparison)
    \item[$\square$] API credit budget allocated (\$50--100 recommended for full program)
    \item[$\square$] Environment variables configured for secure key storage
\end{itemize}

\textbf{Learning Infrastructure:}
\begin{itemize}
    \item[$\square$] Note-taking system established (Notion, Obsidian, etc.)
    \item[$\square$] Calendar blocked for study sessions
    \item[$\square$] Community access (Discord, Slack, or study group)
    \item[$\square$] Core textbooks acquired (see Resources chapter)
\end{itemize}
\end{selfevalbox}

\section{Mindset Preparation}

Success in this program requires embracing several key mindsets:

\subsection{Empirical Iteration}

LLM behavior is inherently probabilistic and sometimes surprising. Effective prompt engineers adopt an experimental mindset, systematically testing hypotheses rather than assuming outcomes. Every prompt is a hypothesis to be validated, not a certainty.

\subsection{Provider Agnosticism}

While you may develop on one platform, the principles taught here transfer across all major providers. Avoid over-fitting your mental models to any single provider's quirks or features. The goal is portable expertise.

\subsection{Continuous Learning}

The LLM landscape evolves rapidly. Techniques that are state-of-the-art today may be superseded within months. This program provides foundational skills and frameworks for ongoing adaptation, not a static body of knowledge.

\subsection{Production Orientation}

From Day 1, think about how techniques would work in production environments. Consider edge cases, failure modes, costs, and user experience even during learning exercises.

% ============================================================
% PART II: DETAILED LEARNING PATH
% ============================================================
\part{Detailed Learning Path}

% ============================================================
% CHAPTER: PHASE 1 DETAILED ROADMAP
% ============================================================
\chapter{Phase 1: Foundation Mastery}
\label{ch:phase1-detailed}

\begin{timebox}
\textbf{Duration:} 4 weeks \\
\textbf{Weekly Commitment:} 10--12 hours \\
\textbf{Total Hours:} 40--48 hours \\
\textbf{Delivery:} Personal Prompt Cookbook
\end{timebox}

\section{Phase 1 Learning Path Visualization}

\begin{center}
\begin{tikzpicture}[
    scale=0.85,
    week/.style={draw, rounded corners, minimum width=3cm, minimum height=1.5cm, align=center, fill=phase1color!15, draw=phase1color},
    topic/.style={draw, rounded corners, minimum width=2.2cm, minimum height=0.6cm, align=center, font=\scriptsize, fill=white},
    milestone/.style={draw, rounded corners, minimum width=2.5cm, minimum height=0.8cm, align=center, font=\scriptsize\bfseries, fill=successgreen!20, draw=successgreen},
    arrow/.style={->, thick, darkgray}
]

% Weeks
\node[week] (w1) at (0,0) {\textbf{Week 1}\\Fundamentals};
\node[week] (w2) at (4.5,0) {\textbf{Week 2}\\Patterns};
\node[week] (w3) at (9,0) {\textbf{Week 3}\\Evaluation};
\node[week] (w4) at (13.5,0) {\textbf{Week 4}\\Specialization};

% Week 1 topics
\node[topic] at (0,-1.5) {LLM Mechanics};
\node[topic] at (0,-2.3) {Prompt Anatomy};
\node[topic] at (0,-3.1) {Core Techniques};

% Week 2 topics
\node[topic] at (4.5,-1.5) {Chain-of-Thought};
\node[topic] at (4.5,-2.3) {Few-Shot Learning};
\node[topic] at (4.5,-3.1) {Self-Consistency};

% Week 3 topics
\node[topic] at (9,-1.5) {Metrics Design};
\node[topic] at (9,-2.3) {Test Set Creation};
\node[topic] at (9,-3.1) {A/B Testing};

% Week 4 topics
\node[topic] at (13.5,-1.5) {Code Generation};
\node[topic] at (13.5,-2.3) {Structured Output};
\node[topic] at (13.5,-3.1) {Multi-Modal};

% Milestone
\node[milestone] at (6.75,-4.5) {Checkpoint: First 10 Patterns};
\node[milestone] at (13.5,-4.5) {Capstone: Prompt Cookbook};

% Arrows
\draw[arrow] (w1) -- (w2);
\draw[arrow] (w2) -- (w3);
\draw[arrow] (w3) -- (w4);

\end{tikzpicture}
\end{center}

\section{Week 1: Prompt Engineering Fundamentals}

\subsection{Learning Objectives}

By the end of Week 1, you will:

\begin{itemize}[leftmargin=1.5cm]
    \item Understand how language models process and generate text at a conceptual level
    \item Identify the components of effective prompts and their purposes
    \item Apply fundamental prompting techniques with confidence
    \item Configure generation parameters appropriately for different tasks
    \item Recognize the capabilities and limitations of current LLM technology
\end{itemize}

\subsection{Day-by-Day Breakdown}

\begin{weekbox}[Days 1--2: Understanding Language Models]
\textbf{Core Concepts:}
\begin{itemize}
    \item Tokenization: How text becomes numbers; why token boundaries matter
    \item Context windows: Working within limits; strategies for context management
    \item Probabilistic generation: Outputs as samples from distributions
    \item Instruction tuning: How models learn to follow directions (RLHF, Constitutional AI concepts)
    \item Model families: Understanding capability tiers without vendor lock-in
\end{itemize}

\textbf{Practical Activities:}
\begin{enumerate}
    \item Experiment with a tokenizer to understand token boundaries
    \item Test identical prompts across different model sizes, noting capability differences
    \item Explore generation parameters (temperature, top-p) with the same prompt
\end{enumerate}

\textbf{Time Investment:} 4--5 hours
\end{weekbox}

\begin{weekbox}[Days 3--4: Anatomy of Effective Prompts]
\textbf{Core Concepts:}
\begin{itemize}
    \item Prompt structure: Role, context, instruction, format, constraints
    \item Clarity techniques: Specificity, explicit requirements, unambiguous language
    \item Output formatting: JSON, XML, Markdown, custom structures
    \item Positive and negative constraints: What to include versus avoid
    \item Delimiters and structure: Organizing complex prompts
\end{itemize}

\textbf{Practical Activities:}
\begin{enumerate}
    \item Transform 5 vague prompts into precise, well-structured versions
    \item Create a ``prompt anatomy'' template for future use
    \item Build a prompt that produces valid JSON output reliably
\end{enumerate}

\textbf{Time Investment:} 4--5 hours
\end{weekbox}

\begin{weekbox}[Days 5--7: Core Prompting Techniques]
\textbf{Techniques to Master:}

\begin{longtable}{p{3.5cm}p{10cm}}
\toprule
\textbf{Technique} & \textbf{Implementation Focus} \\
\midrule
Role Prompting & Assign personas to influence response style, depth, and perspective \\
\midrule
Context Setting & Provide relevant background to ground the model's responses \\
\midrule
Instruction Clarity & Write unambiguous, specific, actionable instructions \\
\midrule
Constraint Specification & Define boundaries: length, scope, format, style \\
\midrule
Output Formatting & Specify exact output structure for reliable parsing \\
\midrule
Task Decomposition & Break complex requests into sequential steps \\
\bottomrule
\end{longtable}

\textbf{Practical Activities:}
\begin{enumerate}
    \item Apply each technique to 3 different task types
    \item Compare outputs with and without each technique
    \item Document which techniques work best for which scenarios
\end{enumerate}

\textbf{Time Investment:} 3--4 hours
\end{weekbox}

\begin{selfevalbox}
\textbf{Week 1 Self-Assessment:}
\begin{itemize}
    \item[$\square$] I can explain why the same prompt might produce different outputs
    \item[$\square$] I can identify at least 5 components of a well-structured prompt
    \item[$\square$] I can transform a vague request into a precise prompt
    \item[$\square$] I can reliably produce structured output (JSON) from prompts
    \item[$\square$] I understand when to use high vs. low temperature settings
    \item[$\square$] I have documented at least 6 fundamental prompting techniques
\end{itemize}
\end{selfevalbox}

\section{Week 2: Advanced Prompting Patterns}

\subsection{Learning Objectives}

By the end of Week 2, you will:

\begin{itemize}[leftmargin=1.5cm]
    \item Implement chain-of-thought prompting and its variants effectively
    \item Design few-shot examples that maximize model performance
    \item Apply self-consistency techniques for high-stakes decisions
    \item Use critique-and-revise patterns to improve output quality
    \item Understand when to apply each advanced pattern
\end{itemize}

\subsection{Pattern Deep Dives}

\subsubsection{Chain-of-Thought (CoT) Prompting}

Chain-of-thought prompting enables complex reasoning by having the model generate intermediate steps before reaching conclusions.

\begin{longtable}{p{3.5cm}p{4.5cm}p{6cm}}
\toprule
\textbf{CoT Variant} & \textbf{Trigger Phrase} & \textbf{Best Use Case} \\
\midrule
\endhead
Zero-Shot CoT & ``Let's think step by step'' & Quick reasoning enhancement without examples \\
\midrule
Few-Shot CoT & Provide reasoning examples & Complex problems requiring specific reasoning style \\
\midrule
Self-Consistency & Multiple CoT paths & High-stakes decisions requiring confidence \\
\midrule
Tree-of-Thought & Branching exploration & Problems with multiple valid approaches \\
\midrule
Program-Aided & Include code execution & Mathematical or logical verification needed \\
\bottomrule
\end{longtable}

\begin{tipbox}
Chain-of-thought is most effective for multi-step reasoning problems. For simple factual recall or creative writing, CoT may actually reduce quality by introducing unnecessary verbosity. Match the technique to the task.
\end{tipbox}

\subsubsection{Few-Shot Learning Mastery}

\begin{longtable}{p{4cm}p{10cm}}
\toprule
\textbf{Aspect} & \textbf{Best Practice} \\
\midrule
\endhead
Example Count & Start with 3--5 examples; add more only if needed for consistency \\
\midrule
Example Selection & Choose diverse, representative cases; include edge cases sparingly \\
\midrule
Example Ordering & Place most similar examples closest to the actual task \\
\midrule
Format Consistency & Maintain identical structure across all examples \\
\midrule
Negative Examples & Include sparingly when boundary clarification is essential \\
\midrule
Dynamic Selection & Consider retrieval-based example selection for varied inputs \\
\bottomrule
\end{longtable}

\subsubsection{Iterative Refinement Patterns}

\begin{longtable}{p{3.5cm}p{5cm}p{5.5cm}}
\toprule
\textbf{Pattern} & \textbf{Implementation} & \textbf{When to Use} \\
\midrule
\endhead
Critique-Revise & Generate, self-critique, improve & Quality-critical outputs \\
\midrule
Multi-Turn Refinement & Progressive improvement via conversation & Complex, evolving requirements \\
\midrule
Self-Evaluation & Assess output against criteria & Before final delivery \\
\midrule
Adversarial Testing & Probe for weaknesses & Security-sensitive applications \\
\midrule
Constitutional Revision & Apply principle-based evaluation & Alignment-critical outputs \\
\bottomrule
\end{longtable}

\begin{warningbox}
Common Pitfall: Over-engineering prompts with too many patterns at once. Start simple and add complexity only when simpler approaches fail. Each additional technique increases token usage and potential failure points.
\end{warningbox}

\subsection{Week 2 Practical Exercises}

\begin{projectbox}
\textbf{Exercise 2.1: CoT Implementation Study}

Select three distinct problem types:
\begin{enumerate}
    \item Mathematical word problem
    \item Logical reasoning puzzle
    \item Code debugging scenario
\end{enumerate}

For each, implement both zero-shot and few-shot CoT. Document:
\begin{itemize}
    \item Accuracy comparison
    \item Reasoning quality assessment
    \item Token usage difference
    \item Recommendation for when to use each
\end{itemize}

\textbf{Deliverable:} Comparative analysis document with all prompts and outputs.
\end{projectbox}

\begin{projectbox}
\textbf{Exercise 2.2: Few-Shot Engineering}

Create a classification task (sentiment, intent, or category assignment):
\begin{enumerate}
    \item Build few-shot prompts with 1, 3, 5, and 10 examples
    \item Test each variant on 20 test cases
    \item Measure accuracy, consistency, and token cost
    \item Vary example ordering and measure impact
\end{enumerate}

\textbf{Deliverable:} Performance analysis with optimal configuration recommendation.
\end{projectbox}

\begin{selfevalbox}
\textbf{Week 2 Self-Assessment:}
\begin{itemize}
    \item[$\square$] I can implement zero-shot and few-shot chain-of-thought prompting
    \item[$\square$] I can design effective few-shot examples for classification tasks
    \item[$\square$] I understand when self-consistency improves reliability
    \item[$\square$] I can implement a critique-and-revise pipeline
    \item[$\square$] I have documented at least 10 distinct prompting patterns
    \item[$\square$] I can articulate the trade-offs between different patterns
\end{itemize}
\end{selfevalbox}

\section{Week 3: Systematic Evaluation}

\subsection{Learning Objectives}

By the end of Week 3, you will:

\begin{itemize}[leftmargin=1.5cm]
    \item Design appropriate evaluation metrics for different task types
    \item Create comprehensive test sets including edge cases
    \item Implement automated evaluation pipelines
    \item Use LLM-as-judge techniques effectively
    \item Establish baselines and track improvements systematically
\end{itemize}

\subsection{Evaluation Framework}

\subsubsection{Metric Selection by Task Type}

\begin{longtable}{p{3.5cm}p{4cm}p{6.5cm}}
\toprule
\textbf{Task Type} & \textbf{Primary Metrics} & \textbf{Implementation Notes} \\
\midrule
\endhead
Classification & Accuracy, F1, Precision, Recall & Use stratified test sets; report per-class metrics \\
\midrule
Extraction & Exact Match, Token F1 & Define matching criteria carefully; handle partial matches \\
\midrule
Generation & BLEU, ROUGE, BERTScore & Choose based on what aspect matters (fluency, coverage, similarity) \\
\midrule
Summarization & ROUGE-L, factual accuracy & Include human evaluation for faithfulness \\
\midrule
Code Generation & Execution pass rate, correctness & Automated testing against test cases \\
\midrule
Open-Ended & LLM-as-judge, human rating & Define rubrics explicitly; ensure consistency \\
\bottomrule
\end{longtable}

\subsubsection{Test Set Construction}

A high-quality test set includes:

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{Representative Cases (60\%):} Typical inputs the system will encounter
    \item \textbf{Edge Cases (20\%):} Boundary conditions, unusual inputs, extreme values
    \item \textbf{Adversarial Cases (10\%):} Inputs designed to confuse or break the system
    \item \textbf{Regression Cases (10\%):} Previously failed inputs that have been fixed
\end{itemize}

\subsubsection{LLM-as-Judge Implementation}

\begin{tipbox}
When using LLMs to evaluate other LLM outputs, provide explicit rubrics with scoring criteria. Include examples of outputs at each score level. Consider using a different model family for evaluation to avoid systematic biases.
\end{tipbox}

\subsection{Week 3 Practical Exercises}

\begin{projectbox}
\textbf{Exercise 3.1: Golden Test Set Creation}

For a prompt developed in Weeks 1--2:
\begin{enumerate}
    \item Create 30+ test cases with expected outputs
    \item Categorize by case type (representative, edge, adversarial)
    \item Implement automated scoring for applicable metrics
    \item Run baseline evaluation and document results
\end{enumerate}

\textbf{Deliverable:} Test set file (JSON/YAML) plus evaluation script with baseline results.
\end{projectbox}

\begin{projectbox}
\textbf{Exercise 3.2: LLM-as-Judge Pipeline}

Build an evaluation system that:
\begin{enumerate}
    \item Defines clear scoring rubrics (1--5 scale with descriptions)
    \item Implements LLM-based evaluation with consistent prompts
    \item Calculates inter-rater reliability (if using multiple judge prompts)
    \item Generates summary reports with score distributions
\end{enumerate}

\textbf{Deliverable:} Working LLM-as-judge implementation with documentation.
\end{projectbox}

\section{Week 4: Specialized Prompting Domains}

\subsection{Learning Objectives}

By the end of Week 4, you will:

\begin{itemize}[leftmargin=1.5cm]
    \item Apply prompting techniques to code generation tasks effectively
    \item Produce reliable structured outputs (JSON, XML, YAML)
    \item Work with vision-language models for multi-modal tasks
    \item Adapt prompting strategies for domain-specific applications
    \item Complete the Phase 1 capstone project
\end{itemize}

\subsection{Code Generation Patterns}

\begin{longtable}{p{4cm}p{10cm}}
\toprule
\textbf{Pattern} & \textbf{Implementation} \\
\midrule
\endhead
Specification-First & Provide detailed requirements including edge cases before requesting code \\
\midrule
Test-Driven & Supply test cases; ask for implementation that passes them \\
\midrule
Incremental Development & Build functionality step-by-step, verifying each step \\
\midrule
Pseudo-code Scaffolding & Outline structure first, then request implementation \\
\midrule
Code Review Prompting & Analyze and improve existing code systematically \\
\midrule
Documentation Generation & Generate docstrings, comments, and READMEs from code \\
\bottomrule
\end{longtable}

\subsection{Structured Output Reliability}

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{Schema Definition:} Always provide explicit JSON schemas or format examples
    \item \textbf{Validation:} Implement parsing with error handling; retry on malformed output
    \item \textbf{Mode Selection:} Use provider-specific structured output modes when available
    \item \textbf{Fallback Strategies:} Define behavior when structured output fails
\end{itemize}

\subsection{Multi-Modal Prompting}

When working with vision-language models:

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{Image Description:} Be specific about what aspects of the image to analyze
    \item \textbf{Grounding:} Reference image regions explicitly (``in the upper left,'' ``the red object'')
    \item \textbf{Task Clarity:} Distinguish between description, analysis, and generation tasks
    \item \textbf{Limitations:} Understand model limitations with text in images, fine details, spatial reasoning
\end{itemize}

\section{Phase 1 Capstone: Personal Prompt Cookbook}

\begin{milestonebox}
\textbf{Deliverable: Personal Prompt Cookbook}

Compile a comprehensive prompt cookbook containing:

\textbf{Part 1: Pattern Library (20--30 patterns)}
\begin{itemize}
    \item Pattern name and category
    \item Description and use cases
    \item Template with placeholders
    \item 2--3 concrete examples with outputs
    \item Known limitations and failure modes
    \item Provider-specific notes (if any)
\end{itemize}

\textbf{Part 2: Evaluation Framework}
\begin{itemize}
    \item Test cases for representative patterns
    \item Automated evaluation scripts
    \item Quality metrics and thresholds
    \item LLM-as-judge rubrics
\end{itemize}

\textbf{Part 3: Personal Insights}
\begin{itemize}
    \item Best practices discovered through experimentation
    \item Anti-patterns to avoid
    \item Decision framework for pattern selection
\end{itemize}

\textbf{Format:} GitHub repository with Markdown documentation and supporting code.
\end{milestonebox}

\begin{assessmentbox}
\textbf{Phase 1 Assessment Criteria:}

\begin{tabularx}{\textwidth}{Xc}
\toprule
\textbf{Criterion} & \textbf{Weight} \\
\midrule
Pattern variety and coverage (20+ distinct patterns) & 25\% \\
Quality of documentation and examples & 25\% \\
Evaluation implementation and rigor & 20\% \\
Personal insights and original contributions & 15\% \\
Code quality and organization & 15\% \\
\bottomrule
\end{tabularx}

\textbf{Passing Threshold:} 70\% overall, with no criterion below 50\%.
\end{assessmentbox}

% ============================================================
% CHAPTER: PHASE 2 DETAILED ROADMAP
% ============================================================
\chapter{Phase 2: Application Development}
\label{ch:phase2-detailed}

\begin{timebox}
\textbf{Duration:} 6 weeks \\
\textbf{Weekly Commitment:} 10--12 hours \\
\textbf{Total Hours:} 60--72 hours \\
\textbf{Delivery:} 2--3 Portfolio Applications
\end{timebox}

\section{Phase 2 Learning Path Visualization}

\begin{center}
\begin{tikzpicture}[
    scale=0.75,
    week/.style={draw, rounded corners, minimum width=2.2cm, minimum height=1.3cm, align=center, fill=phase2color!15, draw=phase2color, font=\small},
    topic/.style={draw, rounded corners, minimum width=1.8cm, minimum height=0.5cm, align=center, font=\tiny, fill=white},
    milestone/.style={draw, rounded corners, minimum width=2cm, minimum height=0.6cm, align=center, font=\tiny\bfseries, fill=successgreen!20, draw=successgreen},
    arrow/.style={->, thick, darkgray}
]

% Weeks
\node[week] (w5) at (0,0) {\textbf{Week 5}\\Architecture};
\node[week] (w6) at (3,0) {\textbf{Week 6}\\Tools};
\node[week] (w7) at (6,0) {\textbf{Week 7}\\RAG I};
\node[week] (w8) at (9,0) {\textbf{Week 8}\\RAG II};
\node[week] (w9) at (12,0) {\textbf{Week 9}\\Agents};
\node[week] (w10) at (15,0) {\textbf{Week 10}\\Projects};

% Topics
\node[topic] at (0,-1.3) {Patterns};
\node[topic] at (0,-1.9) {State Mgmt};

\node[topic] at (3,-1.3) {Function Call};
\node[topic] at (3,-1.9) {Tool Design};

\node[topic] at (6,-1.3) {Chunking};
\node[topic] at (6,-1.9) {Embedding};

\node[topic] at (9,-1.3) {Retrieval};
\node[topic] at (9,-1.9) {Reranking};

\node[topic] at (12,-1.3) {Memory};
\node[topic] at (12,-1.9) {Planning};

\node[topic] at (15,-1.3) {Integration};
\node[topic] at (15,-1.9) {Deployment};

% Milestones
\node[milestone] at (4.5,-2.8) {Project 1: Tool Agent};
\node[milestone] at (10.5,-2.8) {Project 2: RAG System};
\node[milestone] at (15,-2.8) {Project 3: Full App};

% Arrows
\draw[arrow] (w5) -- (w6);
\draw[arrow] (w6) -- (w7);
\draw[arrow] (w7) -- (w8);
\draw[arrow] (w8) -- (w9);
\draw[arrow] (w9) -- (w10);

\end{tikzpicture}
\end{center}

\section{Week 5: LLM Application Architecture}

\subsection{Learning Objectives}

\begin{itemize}[leftmargin=1.5cm]
    \item Identify and select appropriate architectural patterns for LLM applications
    \item Design component interactions for maintainability and scalability
    \item Implement proper state management and conversation handling
    \item Apply caching strategies for cost and latency optimization
    \item Handle errors gracefully in LLM-powered systems
\end{itemize}

\subsection{Architecture Pattern Catalog}

\begin{longtable}{p{3cm}p{4.5cm}p{3cm}p{3.5cm}}
\toprule
\textbf{Pattern} & \textbf{Description} & \textbf{Complexity} & \textbf{Use Cases} \\
\midrule
\endhead
Direct API & Simple request-response & \levelindicator{0} & Single-turn tasks, MVP \\
\midrule
Prompt Chain & Sequential prompts & \levelindicator{1} & Multi-step workflows \\
\midrule
Router & Classify and delegate & \levelindicator{1} & Multi-intent systems \\
\midrule
RAG Pipeline & Retrieve then generate & \levelindicator{2} & Knowledge-based QA \\
\midrule
Agent Loop & Plan-act-observe cycle & \levelindicator{2} & Autonomous tasks \\
\midrule
Multi-Agent & Coordinated specialists & \levelindicator{3} & Complex workflows \\
\midrule
Human-in-Loop & LLM + human verification & \levelindicator{2} & High-stakes decisions \\
\bottomrule
\end{longtable}

\subsection{Component Design Principles}

\subsubsection{Prompt Management}

\begin{itemize}[leftmargin=1.5cm]
    \item Store prompts in version-controlled templates, not hard-coded strings
    \item Implement parameterization for dynamic content injection
    \item Maintain separate prompts for development, testing, and production
    \item Document prompt purpose, expected inputs, and output format
\end{itemize}

\subsubsection{Context Management}

\begin{itemize}[leftmargin=1.5cm]
    \item Implement context window tracking to prevent truncation
    \item Design summarization strategies for long conversations
    \item Consider sliding window, summarization, or hybrid approaches
    \item Handle context overflow gracefully with clear user feedback
\end{itemize}

\subsubsection{Response Handling}

\begin{itemize}[leftmargin=1.5cm]
    \item Parse and validate outputs before use
    \item Implement retry logic with exponential backoff
    \item Define fallback behaviors for malformed responses
    \item Log all requests and responses for debugging
\end{itemize}

\begin{projectbox}
\textbf{Exercise 5.1: Architecture Design Document}

Design an LLM application (choose: customer support bot, code review assistant, or research summarizer):

\begin{enumerate}
    \item Create system context and component diagrams
    \item Document data flows and API contracts
    \item Define error handling and fallback strategies
    \item Estimate costs at different usage levels
    \item Plan for horizontal scaling
\end{enumerate}

\textbf{Deliverable:} Architecture design document with diagrams.
\end{projectbox}

\section{Week 6: Tool Integration and Function Calling}

\subsection{Learning Objectives}

\begin{itemize}[leftmargin=1.5cm]
    \item Implement function calling across different provider APIs
    \item Design tool schemas that guide model behavior effectively
    \item Handle parallel and sequential tool execution
    \item Manage errors and edge cases in tool chains
    \item Apply security best practices for tool access
\end{itemize}

\subsection{Tool Design Best Practices}

\begin{longtable}{p{4cm}p{10cm}}
\toprule
\textbf{Aspect} & \textbf{Best Practice} \\
\midrule
\endhead
Naming Convention & Use clear, action-oriented names: \texttt{search\_database}, \texttt{send\_email}, \texttt{calculate\_total} \\
\midrule
Description Quality & Provide detailed descriptions of when, why, and how to use each tool \\
\midrule
Parameter Design & Well-typed parameters with clear descriptions, constraints, and examples \\
\midrule
Return Format & Consistent, parseable return formats; include both data and status \\
\midrule
Error Messages & Informative errors the LLM can interpret and respond to appropriately \\
\midrule
Scope Limitation & Minimal necessary permissions; explicit boundaries on what tools can do \\
\midrule
Confirmation Gates & Require confirmation for destructive or expensive operations \\
\bottomrule
\end{longtable}

\begin{warningbox}
Security Warning: Tools that access external systems (databases, APIs, file systems) must validate all LLM-generated parameters. Never pass LLM output directly to system commands or database queries without sanitization. Implement rate limiting and audit logging for all tool invocations.
\end{warningbox}

\begin{projectbox}
\textbf{Project 1: Multi-Tool Agent}

Build an agent with access to 4+ tools:
\begin{itemize}
    \item Information retrieval tool (web search or database)
    \item Computation tool (calculator, data analysis)
    \item Communication tool (email draft, notification)
    \item File/data tool (read/write structured data)
\end{itemize}

Requirements:
\begin{itemize}
    \item Proper tool selection based on user intent
    \item Sequential and parallel tool execution
    \item Error handling and recovery
    \item Conversation history integration
    \item Security validation on all inputs
\end{itemize}

\textbf{Deliverable:} Working agent with documentation and test suite.
\end{projectbox}

\section{Weeks 7--8: Retrieval-Augmented Generation (RAG)}

\subsection{Learning Objectives}

\begin{itemize}[leftmargin=1.5cm]
    \item Design and implement complete RAG pipelines
    \item Apply appropriate document chunking strategies
    \item Select and configure embedding models and vector stores
    \item Implement retrieval optimization techniques
    \item Evaluate and iterate on RAG system quality
\end{itemize}

\subsection{RAG Pipeline Architecture}

\begin{center}
\begin{tikzpicture}[
    node distance=1.5cm,
    block/.style={draw, rounded corners, minimum width=2.5cm, minimum height=1cm, align=center, fill=phase2color!10},
    arrow/.style={->, thick}
]

% Ingestion Pipeline
\node[block] (docs) {Documents};
\node[block, right=of docs] (chunk) {Chunking};
\node[block, right=of chunk] (embed) {Embedding};
\node[block, right=of embed] (store) {Vector Store};

% Query Pipeline
\node[block, below=2cm of docs] (query) {User Query};
\node[block, right=of query] (qembed) {Query\\Embedding};
\node[block, right=of qembed] (retrieve) {Retrieval};
\node[block, right=of retrieve] (rerank) {Reranking};
\node[block, below=2cm of rerank] (llm) {LLM\\Generation};
\node[block, left=of llm] (response) {Response};

% Arrows
\draw[arrow] (docs) -- (chunk);
\draw[arrow] (chunk) -- (embed);
\draw[arrow] (embed) -- (store);

\draw[arrow] (query) -- (qembed);
\draw[arrow] (qembed) -- (retrieve);
\draw[arrow] (retrieve) -- (rerank);
\draw[arrow] (store) -- (retrieve);
\draw[arrow] (rerank) -- (llm);
\draw[arrow] (llm) -- (response);

% Labels
\node[above=0.3cm of chunk, font=\scriptsize] {Ingestion Pipeline};
\node[above=0.3cm of retrieve, font=\scriptsize] {Query Pipeline};

\end{tikzpicture}
\end{center}

\subsection{Chunking Strategies}

\begin{longtable}{p{3.5cm}p{5cm}p{5.5cm}}
\toprule
\textbf{Strategy} & \textbf{Description} & \textbf{Best For} \\
\midrule
\endhead
Fixed Size & Split at token/character count & Simple implementation, uniform retrieval \\
\midrule
Recursive & Split hierarchically (paragraph, sentence, word) & Varied document structures \\
\midrule
Semantic & Split at topic boundaries & Topic-coherent retrieval \\
\midrule
Document-Aware & Respect document structure (headers, sections) & Structured documents \\
\midrule
Sliding Window & Overlapping chunks & Context preservation \\
\midrule
Parent-Child & Small chunks linked to larger context & Precise retrieval with broad context \\
\bottomrule
\end{longtable}

\subsection{Retrieval Optimization}

\begin{longtable}{p{4cm}p{10cm}}
\toprule
\textbf{Technique} & \textbf{Implementation} \\
\midrule
\endhead
Hybrid Search & Combine dense (embedding) and sparse (BM25) retrieval \\
\midrule
Query Expansion & Generate multiple query variants to improve recall \\
\midrule
HyDE & Generate hypothetical answer, embed it for retrieval \\
\midrule
Reranking & Use cross-encoder to reorder initial retrieval results \\
\midrule
Metadata Filtering & Pre-filter by date, source, category before semantic search \\
\midrule
Multi-Query & Generate diverse queries from single user input \\
\bottomrule
\end{longtable}

\begin{projectbox}
\textbf{Project 2: RAG-Based Document Q\&A}

Build a complete RAG system:
\begin{enumerate}
    \item Document ingestion pipeline (PDF, Markdown, HTML)
    \item Configurable chunking with multiple strategies
    \item Vector storage with any provider-agnostic approach
    \item Conversational interface with context tracking
    \item Source citation in responses
    \item Evaluation suite with retrieval and generation metrics
\end{enumerate}

Optimization Phase:
\begin{itemize}
    \item Compare chunking strategies on same document set
    \item Implement and compare reranking approaches
    \item Test hybrid retrieval vs. dense-only
    \item Document performance trade-offs
\end{itemize}

\textbf{Deliverable:} Working RAG application with optimization analysis report.
\end{projectbox}

\section{Week 9: Conversational Agents and Memory}

\subsection{Learning Objectives}

\begin{itemize}[leftmargin=1.5cm]
    \item Design memory architectures for different conversation types
    \item Implement context summarization and compression
    \item Build agents with consistent personas across sessions
    \item Handle entity tracking and reference resolution
    \item Manage long-term memory persistence
\end{itemize}

\subsection{Memory Architecture Comparison}

\begin{longtable}{p{3cm}p{4cm}p{3.5cm}p{3.5cm}}
\toprule
\textbf{Type} & \textbf{Implementation} & \textbf{Pros} & \textbf{Cons} \\
\midrule
\endhead
Buffer & Full history in context & Complete context & Context limit hit quickly \\
\midrule
Window & Last N turns only & Predictable tokens & Loses early context \\
\midrule
Summary & Compressed history & Handles long chats & Information loss \\
\midrule
Entity & Track entities/facts & Efficient storage & Complex implementation \\
\midrule
Vector & Semantic retrieval & Relevant recall & Retrieval quality varies \\
\midrule
Hybrid & Combined approaches & Balanced trade-offs & System complexity \\
\bottomrule
\end{longtable}

\subsection{Persona Consistency Techniques}

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{System Prompt Anchoring:} Define persona characteristics in system prompt
    \item \textbf{Fact Sheets:} Maintain explicit lists of persona facts for reference
    \item \textbf{Consistency Checking:} Validate responses against persona constraints
    \item \textbf{Memory Integration:} Include persona facts in memory retrieval
\end{itemize}

\section{Week 10: Integration and Capstone Projects}

\subsection{Portfolio Project Requirements}

Complete at least two portfolio-quality applications:

\begin{trackbox}[\faIcon{robot} Track A: Conversational Agent]{trackA}
\begin{itemize}
    \item Multi-turn chatbot with memory
    \item Consistent persona across sessions
    \item Tool integration for actions
    \item Conversation analytics
\end{itemize}
\end{trackbox}

\begin{trackbox}[\faIcon{search} Track B: Knowledge System]{trackB}
\begin{itemize}
    \item Document ingestion pipeline
    \item Semantic search interface
    \item Answer with citations
    \item Evaluation dashboard
\end{itemize}
\end{trackbox}

\begin{trackbox}[\faIcon{database} Track C: Data Interface]{trackC}
\begin{itemize}
    \item Natural language to structured queries
    \item Result visualization
    \item Query explanation
    \item Multi-source integration
\end{itemize}
\end{trackbox}

\begin{milestonebox}
\textbf{Phase 2 Capstone Deliverables:}

For each project:
\begin{itemize}
    \item Working application deployed (local or cloud)
    \item Source code in version control
    \item Architecture documentation with diagrams
    \item README with setup and usage instructions
    \item Evaluation suite with test cases
    \item Performance metrics and analysis
    \item Provider abstraction (works with multiple LLMs)
\end{itemize}
\end{milestonebox}

\begin{assessmentbox}
\textbf{Phase 2 Assessment Criteria:}

\begin{tabularx}{\textwidth}{Xc}
\toprule
\textbf{Criterion} & \textbf{Weight} \\
\midrule
Application functionality and completeness & 30\% \\
Code quality, architecture, and maintainability & 25\% \\
Documentation quality (README, architecture docs) & 15\% \\
Evaluation implementation and coverage & 15\% \\
Provider abstraction and portability & 15\% \\
\bottomrule
\end{tabularx}

\textbf{Passing Threshold:} 70\% overall, with working functionality required.
\end{assessmentbox}

% ============================================================
% CHAPTER: PHASE 3 DETAILED ROADMAP
% ============================================================
\chapter{Phase 3: Production Engineering}
\label{ch:phase3-detailed}

\begin{timebox}
\textbf{Duration:} 4 weeks \\
\textbf{Weekly Commitment:} 10--12 hours \\
\textbf{Total Hours:} 40--48 hours \\
\textbf{Delivery:} Production Playbook
\end{timebox}

\section{Phase 3 Learning Path Visualization}

\begin{center}
\begin{tikzpicture}[
    scale=0.85,
    week/.style={draw, rounded corners, minimum width=3cm, minimum height=1.5cm, align=center, fill=phase3color!15, draw=phase3color},
    topic/.style={draw, rounded corners, minimum width=2.2cm, minimum height=0.6cm, align=center, font=\scriptsize, fill=white},
    milestone/.style={draw, rounded corners, minimum width=3cm, minimum height=0.8cm, align=center, font=\scriptsize\bfseries, fill=successgreen!20, draw=successgreen}
]

% Weeks
\node[week] (w11) at (0,0) {\textbf{Week 11}\\System Design};
\node[week] (w12) at (4.5,0) {\textbf{Week 12}\\Evaluation};
\node[week] (w13) at (9,0) {\textbf{Week 13}\\Operations};
\node[week] (w14) at (13.5,0) {\textbf{Week 14}\\Security};

% Topics
\node[topic] at (0,-1.5) {Decision Framework};
\node[topic] at (0,-2.3) {Cost Modeling};
\node[topic] at (0,-3.1) {Scaling Strategy};

\node[topic] at (4.5,-1.5) {Eval Pipelines};
\node[topic] at (4.5,-2.3) {LLM-as-Judge};
\node[topic] at (4.5,-3.1) {A/B Testing};

\node[topic] at (9,-1.5) {Observability};
\node[topic] at (9,-2.3) {Incident Response};
\node[topic] at (9,-3.1) {Feedback Loops};

\node[topic] at (13.5,-1.5) {Prompt Injection};
\node[topic] at (13.5,-2.3) {Data Privacy};
\node[topic] at (13.5,-3.1) {Guardrails};

% Milestone
\node[milestone] at (6.75,-4.5) {Production Readiness Review};

\end{tikzpicture}
\end{center}

\section{Week 11: System Design for Production}

\subsection{Learning Objectives}

\begin{itemize}[leftmargin=1.5cm]
    \item Apply decision frameworks for prompting vs. fine-tuning vs. RAG
    \item Model costs accurately across different usage scenarios
    \item Design systems for latency, reliability, and scalability requirements
    \item Select appropriate models based on comprehensive criteria
    \item Plan multi-provider strategies for resilience
\end{itemize}

\subsection{Decision Framework: Prompting vs. Fine-Tuning vs. RAG}

\begin{longtable}{p{3cm}p{3.5cm}p{3.5cm}p{4cm}}
\toprule
\textbf{Factor} & \textbf{Better Prompting} & \textbf{Fine-Tuning} & \textbf{RAG} \\
\midrule
\endhead
Knowledge Type & General/static & Style/format & Dynamic/external \\
\midrule
Data Volume & Any & 100s--1000s examples & Any corpus size \\
\midrule
Update Frequency & Immediate & Requires retraining & Real-time possible \\
\midrule
Iteration Speed & Very fast & Days to weeks & Fast (retrieval tuning) \\
\midrule
Cost Structure & Per-token & Training + inference & Storage + retrieval + inference \\
\midrule
Attribution & None & None & Source citation possible \\
\midrule
Hallucination & Model-dependent & Reduced for trained domain & Reduced with good retrieval \\
\bottomrule
\end{longtable}

\subsection{Cost Modeling Framework}

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{Input Costs:} System prompt + context + user input tokens
    \item \textbf{Output Costs:} Generated response tokens
    \item \textbf{RAG Costs:} Embedding generation + vector storage + retrieval queries
    \item \textbf{Tool Costs:} External API calls, compute for tool execution
    \item \textbf{Hidden Costs:} Retry attempts, evaluation runs, logging storage
\end{itemize}

\begin{tipbox}
Cost Optimization Strategies:
\begin{enumerate}
    \item Cache frequent queries and their responses
    \item Use smaller models for classification/routing, larger for generation
    \item Compress system prompts; eliminate redundancy
    \item Implement early stopping for streaming responses when appropriate
    \item Batch requests when real-time response isn't required
\end{enumerate}
\end{tipbox}

\section{Week 12: Evaluation at Scale}

\subsection{Learning Objectives}

\begin{itemize}[leftmargin=1.5cm]
    \item Build comprehensive evaluation pipelines for production systems
    \item Implement continuous evaluation with automated alerting
    \item Design and execute A/B tests for prompt changes
    \item Detect and respond to model drift and regression
    \item Balance automated and human evaluation appropriately
\end{itemize}

\subsection{Multi-Level Evaluation Framework}

\begin{longtable}{p{2.5cm}p{4cm}p{4cm}p{3.5cm}}
\toprule
\textbf{Level} & \textbf{What to Measure} & \textbf{How to Measure} & \textbf{Frequency} \\
\midrule
\endhead
Component & Individual prompt quality & Unit tests, golden sets & Every change \\
\midrule
Pipeline & End-to-end correctness & Integration tests, traces & Daily \\
\midrule
System & User-facing quality & A/B tests, user feedback & Weekly \\
\midrule
Business & Impact on KPIs & Analytics, conversion & Monthly \\
\bottomrule
\end{longtable}

\subsection{LLM-as-Judge Best Practices}

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{Explicit Rubrics:} Define 1--5 scale with detailed descriptions for each level
    \item \textbf{Calibration Examples:} Include examples of outputs at each score level
    \item \textbf{Multiple Dimensions:} Evaluate relevance, accuracy, completeness, safety separately
    \item \textbf{Judge Diversity:} Use multiple judge prompts or models to reduce bias
    \item \textbf{Human Calibration:} Regularly compare LLM scores to human ratings
\end{itemize}

\begin{projectbox}
\textbf{Exercise 12.1: Production Evaluation Pipeline}

Build an evaluation system that:
\begin{enumerate}
    \item Runs scheduled evaluations against versioned test sets
    \item Implements multiple evaluation strategies (exact match, semantic, LLM-judge)
    \item Tracks metrics over time with visualization
    \item Alerts on regression beyond thresholds
    \item Integrates with your Phase 2 application
\end{enumerate}

\textbf{Deliverable:} Working evaluation pipeline with dashboard and alerts.
\end{projectbox}

\section{Week 13: LLMOps and Production Operations}

\subsection{Learning Objectives}

\begin{itemize}[leftmargin=1.5cm]
    \item Implement comprehensive observability for LLM systems
    \item Design prompt versioning and deployment workflows
    \item Build feedback collection and processing pipelines
    \item Establish incident response procedures for LLM failures
    \item Create continuous improvement loops from production data
\end{itemize}

\subsection{Observability Stack Components}

\begin{longtable}{p{3.5cm}p{5cm}p{5.5cm}}
\toprule
\textbf{Component} & \textbf{Purpose} & \textbf{Key Metrics} \\
\midrule
\endhead
Request Logging & Audit trail, debugging & Request/response content, latency, tokens \\
\midrule
Distributed Tracing & Multi-step visibility & Span durations, tool calls, retrieval steps \\
\midrule
Performance Metrics & System health & Latency percentiles, throughput, error rates \\
\midrule
Cost Tracking & Budget management & Token usage, cost per request, cost trends \\
\midrule
Quality Monitoring & Output health & Evaluation scores, feedback signals \\
\bottomrule
\end{longtable}

\subsection{Prompt Version Management}

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{Version Control:} Treat prompts as code; store in Git
    \item \textbf{Change Documentation:} Record rationale for each change
    \item \textbf{Staged Rollout:} Deploy to canary, then percentage, then full
    \item \textbf{Rollback Plan:} Maintain ability to instantly revert
    \item \textbf{A/B Testing Integration:} Support running multiple versions simultaneously
\end{itemize}

\section{Week 14: Safety, Security, and Ethics}

\subsection{Learning Objectives}

\begin{itemize}[leftmargin=1.5cm]
    \item Identify and mitigate LLM-specific security vulnerabilities
    \item Implement input validation and output filtering
    \item Design systems that protect user privacy
    \item Apply responsible AI principles in practice
    \item Navigate regulatory requirements (GDPR, AI Act concepts)
\end{itemize}

\subsection{Security Vulnerability Catalog}

\begin{longtable}{p{3cm}p{5cm}p{6cm}}
\toprule
\textbf{Vulnerability} & \textbf{Description} & \textbf{Mitigation Strategies} \\
\midrule
\endhead
Prompt Injection & Malicious input overrides system instructions & Input validation, instruction isolation, output monitoring \\
\midrule
Indirect Injection & Malicious content in retrieved documents & Content sanitization, source validation \\
\midrule
Data Exfiltration & Extracting training data or user data & Output filtering, PII detection, rate limiting \\
\midrule
Jailbreaking & Bypassing safety guardrails & Layered defenses, output classification \\
\midrule
Excessive Agency & Unintended autonomous actions & Permission boundaries, human-in-loop \\
\midrule
Model Extraction & Stealing model via API queries & Rate limiting, query pattern detection \\
\bottomrule
\end{longtable}

\subsection{Defense-in-Depth Strategy}

\begin{enumerate}[leftmargin=1.5cm]
    \item \textbf{Input Layer:} Validate, sanitize, and classify incoming requests
    \item \textbf{Prompt Layer:} Isolate system instructions from user content
    \item \textbf{Model Layer:} Use models with safety training; consider guardrails
    \item \textbf{Output Layer:} Filter, classify, and validate generated content
    \item \textbf{Monitoring Layer:} Detect anomalies, log for audit, alert on violations
\end{enumerate}

\begin{warningbox}
Never trust LLM output for security-sensitive operations. Always validate:
\begin{itemize}
    \item SQL queries before execution
    \item API calls before making requests
    \item File paths before access
    \item Shell commands before running
    \item Any output that will be rendered as HTML
\end{itemize}
\end{warningbox}

\section{Phase 3 Capstone: Production Playbook}

\begin{milestonebox}
\textbf{Deliverable: Production Playbook}

Create a comprehensive operational playbook containing:

\textbf{Part 1: Decision Frameworks}
\begin{itemize}
    \item When to use prompting vs. RAG vs. fine-tuning decision tree
    \item Model selection criteria and comparison matrix
    \item Architecture pattern selection guide
    \item Cost estimation templates and calculators
\end{itemize}

\textbf{Part 2: Quality Management}
\begin{itemize}
    \item Evaluation strategy templates by task type
    \item Test set creation checklists
    \item LLM-as-judge rubric templates
    \item Quality metrics definitions and thresholds
\end{itemize}

\textbf{Part 3: Operations}
\begin{itemize}
    \item Observability setup guides and dashboards
    \item Incident response procedures for common failures
    \item On-call runbooks for LLM-specific issues
    \item Feedback processing workflows
    \item Cost monitoring and optimization playbooks
\end{itemize}

\textbf{Part 4: Safety and Compliance}
\begin{itemize}
    \item Security hardening checklists
    \item Input validation and output filtering guidelines
    \item Privacy protection procedures
    \item Responsible AI review checklist
\end{itemize}

\textbf{Format:} Wiki, Notion, or documentation site with templates and checklists.
\end{milestonebox}

\begin{assessmentbox}
\textbf{Phase 3 Assessment Criteria:}

\begin{tabularx}{\textwidth}{Xc}
\toprule
\textbf{Criterion} & \textbf{Weight} \\
\midrule
Playbook completeness and practicality & 30\% \\
Decision framework clarity and usefulness & 20\% \\
Operational procedures quality & 20\% \\
Security and safety coverage & 15\% \\
Production enhancement of Phase 2 project & 15\% \\
\bottomrule
\end{tabularx}

\textbf{Passing Threshold:} 70\% overall.
\end{assessmentbox}

% ============================================================
% CHAPTER: PHASE 4 MASTERY
% ============================================================
\chapter{Phase 4: Continuous Mastery}
\label{ch:phase4-detailed}

\begin{timebox}
\textbf{Duration:} Ongoing \\
\textbf{Weekly Commitment:} 3--5 hours \\
\textbf{Focus:} Continuous professional development
\end{timebox}

\section{Mastery Development Framework}

Phase 4 represents ongoing professional growth beyond the structured curriculum. Success requires establishing sustainable learning habits and contributing back to the community.

\subsection{Advanced Technique Areas}

\begin{longtable}{p{4cm}p{10cm}}
\toprule
\textbf{Area} & \textbf{Topics to Explore} \\
\midrule
\endhead
Multi-Agent Systems & Agent coordination, task decomposition, specialized roles, consensus mechanisms \\
\midrule
Advanced Reasoning & Constitutional AI, recursive self-improvement, formal verification of outputs \\
\midrule
Efficiency Optimization & Prompt compression, speculative decoding, distillation techniques \\
\midrule
Multimodal Integration & Vision-language-action models, embodied AI, cross-modal reasoning \\
\midrule
Domain Specialization & Vertical-specific techniques (healthcare, legal, finance, code) \\
\midrule
Research Frontiers & Following and implementing academic advances \\
\bottomrule
\end{longtable}

\subsection{Specialization Tracks}

\begin{trackbox}[\faIcon{code} Code Generation Specialist]{trackA}
Deep expertise in code generation, completion, review, and debugging. Focus on IDE integration, multi-file context, and repository-level understanding.
\end{trackbox}

\begin{trackbox}[\faIcon{search} Knowledge Systems Architect]{trackB}
Advanced RAG systems, knowledge graphs, multi-hop reasoning, and enterprise search. Focus on accuracy, attribution, and scale.
\end{trackbox}

\begin{trackbox}[\faIcon{comments} Conversational AI Expert]{trackC}
Advanced dialogue systems, persona consistency, emotional intelligence, and multi-party conversation. Focus on user experience and engagement.
\end{trackbox}

\subsection{Continuous Learning Practices}

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{Weekly:} Read 2--3 new papers or blog posts on LLM techniques
    \item \textbf{Monthly:} Implement one new technique from recent research
    \item \textbf{Quarterly:} Complete a substantial side project exploring new areas
    \item \textbf{Annually:} Contribute to open source, write technical content, or present at meetups
\end{itemize}

\subsection{Community Engagement}

\begin{itemize}[leftmargin=1.5cm]
    \item Participate in prompt engineering communities (Discord, Reddit, forums)
    \item Share learnings through blog posts, talks, or tutorials
    \item Contribute to open-source tools and frameworks
    \item Mentor others earlier in their learning journey
    \item Attend conferences and local AI/ML meetups
\end{itemize}

% ============================================================
% PART III: SUPPORTING MATERIALS
% ============================================================
\part{Supporting Materials}

% ============================================================
% CHAPTER: COMPETENCY FRAMEWORK
% ============================================================
\chapter{Competency Framework}
\label{ch:competencies}

\section{Skill Progression Model}

This framework defines four competency levels, with specific observable behaviors for each.

\begin{center}
\begin{tikzpicture}[
    level/.style={draw, rounded corners, minimum width=3.5cm, minimum height=1.5cm, align=center, font=\small}
]
\node[level, fill=foundationcolor!20, draw=foundationcolor] at (0,0) {\textbf{Foundation}\\Basic prompting\\Single tasks};
\node[level, fill=intermediatecolor!20, draw=intermediatecolor] at (5,0) {\textbf{Intermediate}\\Applications\\Integration};
\node[level, fill=advancedcolor!20, draw=advancedcolor] at (10,0) {\textbf{Advanced}\\Production\\Operations};
\node[level, fill=expertcolor!20, draw=expertcolor] at (15,0) {\textbf{Expert}\\Innovation\\Leadership};

\draw[->, thick, darkgray] (1.8,0) -- (3.2,0);
\draw[->, thick, darkgray] (6.8,0) -- (8.2,0);
\draw[->, thick, darkgray] (11.8,0) -- (13.2,0);
\end{tikzpicture}
\end{center}

\section{Detailed Competency Matrix}

\subsection{Prompt Engineering Competencies}

\begin{longtable}{p{3cm}p{3cm}p{3cm}p{3cm}p{3cm}}
\toprule
\textbf{Skill} & \textbf{Foundation} & \textbf{Intermediate} & \textbf{Advanced} & \textbf{Expert} \\
\midrule
\endhead
Basic Prompting & Write clear prompts; get useful outputs & Consistent quality across tasks & Optimize for edge cases & Teach others; create frameworks \\
\midrule
Pattern Application & Use 5--10 patterns & Master 20+ patterns; select appropriately & Design custom patterns & Pioneer new patterns \\
\midrule
Evaluation & Manual assessment & Automated metrics & Full evaluation pipelines & Design eval frameworks \\
\midrule
Multi-Modal & Basic image+text & Complex multimodal workflows & Production multimodal systems & Multimodal architecture design \\
\bottomrule
\end{longtable}

\subsection{Application Development Competencies}

\begin{longtable}{p{3cm}p{3cm}p{3cm}p{3cm}p{3cm}}
\toprule
\textbf{Skill} & \textbf{Foundation} & \textbf{Intermediate} & \textbf{Advanced} & \textbf{Expert} \\
\midrule
\endhead
Architecture & Understand patterns & Implement patterns & Design novel architectures & Set organizational standards \\
\midrule
Tool Integration & Basic function calls & Multi-tool agents & Complex tool orchestration & Tool framework design \\
\midrule
RAG Systems & Basic retrieval & Optimized pipelines & Production RAG at scale & RAG architecture innovation \\
\midrule
Agents & Simple agents & Stateful conversational & Multi-agent systems & Agent framework design \\
\bottomrule
\end{longtable}

\subsection{Production Engineering Competencies}

\begin{longtable}{p{3cm}p{3cm}p{3cm}p{3cm}p{3cm}}
\toprule
\textbf{Skill} & \textbf{Foundation} & \textbf{Intermediate} & \textbf{Advanced} & \textbf{Expert} \\
\midrule
\endhead
System Design & Understand trade-offs & Make good decisions & Optimize complex systems & Define best practices \\
\midrule
Operations & Basic logging & Full observability & Incident response & Operations strategy \\
\midrule
Security & Aware of risks & Implement protections & Comprehensive hardening & Security architecture \\
\midrule
Cost Management & Track costs & Optimize costs & Cost-efficient design & Cost strategy leadership \\
\bottomrule
\end{longtable}

% ============================================================
% CHAPTER: SELF-ASSESSMENT TOOLS
% ============================================================
\chapter{Self-Assessment Tools}
\label{ch:assessment-tools}

\section{Phase Readiness Checklists}

\subsection{Ready for Phase 2?}

\begin{selfevalbox}
\textbf{Phase 1 Completion Checklist:}

\textbf{Knowledge (must answer ``yes'' to all):}
\begin{itemize}
    \item[$\square$] Can explain how tokenization affects prompt design
    \item[$\square$] Can describe at least 5 prompting patterns and when to use them
    \item[$\square$] Can implement chain-of-thought prompting effectively
    \item[$\square$] Can design few-shot examples for classification tasks
    \item[$\square$] Can create evaluation metrics appropriate to different task types
\end{itemize}

\textbf{Skills (must answer ``yes'' to all):}
\begin{itemize}
    \item[$\square$] Have documented 20+ prompt patterns in personal cookbook
    \item[$\square$] Have implemented automated evaluation for at least one prompt
    \item[$\square$] Have successfully generated structured output (JSON) reliably
    \item[$\square$] Have compared prompt performance across different models
\end{itemize}

\textbf{Artifacts:}
\begin{itemize}
    \item[$\square$] Personal Prompt Cookbook complete and documented
    \item[$\square$] Evaluation scripts functional and tested
\end{itemize}
\end{selfevalbox}

\subsection{Ready for Phase 3?}

\begin{selfevalbox}
\textbf{Phase 2 Completion Checklist:}

\textbf{Knowledge (must answer ``yes'' to all):}
\begin{itemize}
    \item[$\square$] Can explain different LLM application architecture patterns
    \item[$\square$] Can design tool schemas for function calling
    \item[$\square$] Can describe RAG pipeline components and trade-offs
    \item[$\square$] Can explain memory architectures for conversational agents
    \item[$\square$] Can discuss provider abstraction strategies
\end{itemize}

\textbf{Skills (must answer ``yes'' to all):}
\begin{itemize}
    \item[$\square$] Have built working application with tool integration
    \item[$\square$] Have implemented RAG system with retrieval optimization
    \item[$\square$] Have built conversational agent with memory
    \item[$\square$] Applications work with at least two different LLM providers
\end{itemize}

\textbf{Artifacts:}
\begin{itemize}
    \item[$\square$] 2--3 portfolio applications complete and documented
    \item[$\square$] Architecture documentation with diagrams
    \item[$\square$] Evaluation suites for each application
\end{itemize}
\end{selfevalbox}

\subsection{Ready for Phase 4 (Mastery)?}

\begin{selfevalbox}
\textbf{Phase 3 Completion Checklist:}

\textbf{Knowledge (must answer ``yes'' to all):}
\begin{itemize}
    \item[$\square$] Can apply decision frameworks for prompting vs. fine-tuning vs. RAG
    \item[$\square$] Can design comprehensive evaluation pipelines
    \item[$\square$] Can implement observability for LLM systems
    \item[$\square$] Can identify and mitigate LLM security vulnerabilities
    \item[$\square$] Can discuss responsible AI principles and their application
\end{itemize}

\textbf{Skills (must answer ``yes'' to all):}
\begin{itemize}
    \item[$\square$] Have built production evaluation pipeline with alerting
    \item[$\square$] Have implemented security hardening for LLM application
    \item[$\square$] Have created operational runbooks for incident response
    \item[$\square$] Have optimized application for cost and latency
\end{itemize}

\textbf{Artifacts:}
\begin{itemize}
    \item[$\square$] Production Playbook complete with decision frameworks
    \item[$\square$] Phase 2 application enhanced for production
    \item[$\square$] Operational documentation complete
\end{itemize}
\end{selfevalbox}

% ============================================================
% CHAPTER: RESOURCES AND REFERENCES
% ============================================================
\chapter{Resources and References}
\label{ch:resources}

\section{Core Reading List}

\subsection{Primary Textbooks}

\begin{longtable}{p{5cm}p{4cm}p{5cm}}
\toprule
\textbf{Title} & \textbf{Focus Area} & \textbf{Best For} \\
\midrule
\endhead
\textit{Prompt Engineering for Generative AI} & Core prompting skills & Phase 1, structured playbook \\
\midrule
\textit{AI Engineering: Building Applications with Foundation Models} & Full lifecycle & Phase 2--3, production focus \\
\midrule
\textit{LLM Engineer's Handbook} & Operations and infrastructure & Phase 3, production systems \\
\midrule
\textit{Building LLM Powered Applications} & Hands-on development & Phase 2, practical projects \\
\midrule
\textit{Prompt Engineering for LLMs} & Application building & Phase 1--2, comprehensive \\
\bottomrule
\end{longtable}

\subsection{Official Documentation (Provider-Agnostic Approach)}

Study documentation from multiple providers to understand both common principles and provider-specific features:

\begin{itemize}[leftmargin=1.5cm]
    \item Major LLM provider documentation (prompt engineering guides)
    \item API references for function calling and tool use
    \item Best practices and cookbook examples
    \item Safety and usage policies
\end{itemize}

\section{Frameworks and Tools}

\begin{longtable}{p{3.5cm}p{5cm}p{5.5cm}}
\toprule
\textbf{Category} & \textbf{Examples} & \textbf{Use Case} \\
\midrule
\endhead
Orchestration & LangChain, LlamaIndex, Haystack & Building LLM applications \\
\midrule
Vector Stores & Pinecone, Weaviate, Chroma, pgvector & RAG systems \\
\midrule
Observability & LangSmith, Phoenix, Helicone & Monitoring and tracing \\
\midrule
Evaluation & OpenAI Evals, RAGAS, custom & Quality assessment \\
\midrule
Safety & NeMo Guardrails, custom filters & Content safety \\
\bottomrule
\end{longtable}

\section{Key Research Papers}

\begin{enumerate}[leftmargin=1.5cm]
    \item ``Chain-of-Thought Prompting Elicits Reasoning in Large Language Models'' (Wei et al.)
    \item ``Self-Consistency Improves Chain of Thought Reasoning'' (Wang et al.)
    \item ``Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'' (Lewis et al.)
    \item ``Constitutional AI: Harmlessness from AI Feedback'' (Bai et al.)
    \item ``Tree of Thoughts: Deliberate Problem Solving with LLMs'' (Yao et al.)
    \item ``ReAct: Synergizing Reasoning and Acting in Language Models'' (Yao et al.)
    \item ``Toolformer: Language Models Can Teach Themselves to Use Tools'' (Schick et al.)
    \item ``The Shift from Models to Compound AI Systems'' (Berkeley AI Research)
\end{enumerate}

% ============================================================
% CHAPTER: SCHEDULE OPTIONS
% ============================================================
\chapter{Schedule Options and Pacing Guides}
\label{ch:schedule}

\section{Standard Track (14 Weeks)}

\begin{center}
\begin{tabularx}{\textwidth}{c>{\centering\arraybackslash}p{2cm}Xc}
\toprule
\textbf{Week} & \textbf{Phase} & \textbf{Focus} & \textbf{Hours} \\
\midrule
1 & \multirow{4}{*}{\textcolor{phase1color}{\textbf{Phase 1}}} & Fundamentals & 10--12 \\
2 & & Advanced Patterns & 10--12 \\
3 & & Evaluation & 10--12 \\
4 & & Specialization + Capstone & 10--12 \\
\midrule
5 & \multirow{6}{*}{\textcolor{phase2color}{\textbf{Phase 2}}} & Architecture & 10--12 \\
6 & & Tool Integration & 10--12 \\
7 & & RAG Fundamentals & 10--12 \\
8 & & RAG Optimization & 10--12 \\
9 & & Agents \& Memory & 10--12 \\
10 & & Projects + Capstone & 10--12 \\
\midrule
11 & \multirow{4}{*}{\textcolor{phase3color}{\textbf{Phase 3}}} & System Design & 10--12 \\
12 & & Evaluation at Scale & 10--12 \\
13 & & Operations & 10--12 \\
14 & & Safety + Capstone & 10--12 \\
\bottomrule
\end{tabularx}
\end{center}

\section{Accelerated Track (10 Weeks)}

For experienced developers with 15--20 hours per week:

\begin{center}
\begin{tabularx}{\textwidth}{c>{\centering\arraybackslash}p{2cm}Xc}
\toprule
\textbf{Week} & \textbf{Phase} & \textbf{Focus} & \textbf{Hours} \\
\midrule
1 & \multirow{2}{*}{\textcolor{phase1color}{\textbf{Phase 1}}} & Fundamentals + Patterns & 15--20 \\
2 & & Evaluation + Specialization + Capstone & 15--20 \\
\midrule
3 & \multirow{5}{*}{\textcolor{phase2color}{\textbf{Phase 2}}} & Architecture + Tools & 15--20 \\
4 & & RAG Complete & 15--20 \\
5 & & Agents + Memory & 15--20 \\
6 & & Project 1 & 15--20 \\
7 & & Projects 2--3 & 15--20 \\
\midrule
8 & \multirow{3}{*}{\textcolor{phase3color}{\textbf{Phase 3}}} & System Design + Evaluation & 15--20 \\
9 & & Operations + Security & 15--20 \\
10 & & Production Playbook & 15--20 \\
\bottomrule
\end{tabularx}
\end{center}

\section{Extended Track (18+ Weeks)}

For career changers or those with limited weekly time (5--8 hours):

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{Weeks 1--6:} Phase 1 (spread across 6 weeks)
    \item \textbf{Weeks 7--14:} Phase 2 (8 weeks for deeper practice)
    \item \textbf{Weeks 15--18:} Phase 3 (4 weeks at comfortable pace)
    \item Include additional review weeks as needed
\end{itemize}

\section{Self-Paced Guidelines}

\begin{itemize}[leftmargin=1.5cm]
    \item \textbf{Minimum Commitment:} 5 hours per week to maintain momentum
    \item \textbf{Phase Completion:} Complete each phase before starting the next
    \item \textbf{Project-First Option:} Start projects early, learn concepts as needed
    \item \textbf{Milestone Cadence:} Set weekly goals and track progress
    \item \textbf{Community Support:} Join study groups for accountability
\end{itemize}

% ============================================================
% APPENDICES
% ============================================================
\appendix

\chapter{Quick Reference Cards}
\label{app:quickref}

\section{Prompting Pattern Quick Reference}

\begin{longtable}{p{3.5cm}p{4cm}p{6.5cm}}
\toprule
\textbf{Pattern} & \textbf{When to Use} & \textbf{Template Snippet} \\
\midrule
\endhead
Role Prompting & Need expert perspective & ``You are a [role] with expertise in [domain]...'' \\
\midrule
Few-Shot & Need consistent format & ``Examples: [Ex1] [Ex2] Now: [Task]'' \\
\midrule
Chain-of-Thought & Complex reasoning & ``Let's think through this step by step...'' \\
\midrule
Output Format & Structured output needed & ``Respond in JSON format: \{...\}'' \\
\midrule
Constraints & Specific requirements & ``Requirements: 1. ... 2. ... 3. ...'' \\
\midrule
Self-Consistency & High-stakes decisions & Generate N responses, select most common \\
\midrule
Critique-Revise & Quality improvement & ``Critique the above, then improve it'' \\
\midrule
Decomposition & Complex multi-part task & ``Break this into steps: [Task]'' \\
\bottomrule
\end{longtable}

\section{Architecture Decision Quick Reference}

\begin{longtable}{p{4cm}p{4.5cm}p{5.5cm}}
\toprule
\textbf{Requirement} & \textbf{Recommended Pattern} & \textbf{Key Considerations} \\
\midrule
\endhead
Simple Q\&A & Direct API & Lowest complexity, highest latency sensitivity \\
\midrule
Multi-step workflow & Prompt Chain & Define clear handoffs between steps \\
\midrule
Multiple intents & Router Pattern & Classifier accuracy is critical \\
\midrule
External knowledge & RAG Pipeline & Chunk size and retrieval quality matter \\
\midrule
Tool execution & Agent Loop & Tool design and error handling critical \\
\midrule
High stakes & Human-in-Loop & Define clear escalation criteria \\
\bottomrule
\end{longtable}

\section{Security Checklist}

\begin{selfevalbox}
\textbf{Pre-Production Security Checklist:}
\begin{itemize}
    \item[$\square$] Input validation implemented for all user inputs
    \item[$\square$] System prompt isolated from user content
    \item[$\square$] Output filtering for PII and sensitive content
    \item[$\square$] Rate limiting configured
    \item[$\square$] Audit logging enabled
    \item[$\square$] Prompt injection test suite passing
    \item[$\square$] Tool permissions minimized (least privilege)
    \item[$\square$] Human confirmation for destructive actions
    \item[$\square$] Error messages don't leak system details
    \item[$\square$] API keys properly secured (not in code)
\end{itemize}
\end{selfevalbox}

\chapter{Glossary}
\label{app:glossary}

\begin{longtable}{p{4cm}p{10cm}}
\toprule
\textbf{Term} & \textbf{Definition} \\
\midrule
\endhead
Chain-of-Thought (CoT) & Prompting technique that encourages step-by-step reasoning \\
\midrule
Context Window & Maximum number of tokens a model can process in a single request \\
\midrule
Embedding & Dense vector representation of text for semantic similarity \\
\midrule
Few-Shot Learning & Providing examples in the prompt to guide model behavior \\
\midrule
Fine-Tuning & Training a model on custom data to adapt its behavior \\
\midrule
Function Calling & Enabling models to invoke external functions/tools \\
\midrule
Guardrails & Safety mechanisms to constrain model outputs \\
\midrule
Hallucination & Model generating plausible but factually incorrect information \\
\midrule
HyDE & Hypothetical Document Embedding; generating answer for retrieval \\
\midrule
LLM & Large Language Model \\
\midrule
LLMOps & Operations practices specific to LLM systems \\
\midrule
Prompt Injection & Attack where malicious input overrides system instructions \\
\midrule
RAG & Retrieval-Augmented Generation; combining retrieval with generation \\
\midrule
Reranking & Re-ordering retrieved results using a more sophisticated model \\
\midrule
Self-Consistency & Generating multiple responses and selecting most common answer \\
\midrule
System Prompt & Instructions provided to the model that persist across turns \\
\midrule
Temperature & Parameter controlling randomness in model outputs \\
\midrule
Token & Basic unit of text processing in LLMs \\
\midrule
Top-p (Nucleus) & Sampling parameter limiting to most probable tokens \\
\midrule
Vector Store & Database optimized for storing and querying embeddings \\
\midrule
Zero-Shot & Prompting without providing examples \\
\bottomrule
\end{longtable}

% ============================================================
% END DOCUMENT
% ============================================================

\end{document}
